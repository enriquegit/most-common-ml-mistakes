<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Mistake 17: Wasting unlabeled data | MOST COMMON MISTAKES IN MACHINE LEARNING AND HOW TO AVOID THEM</title>
  <meta name="description" content="This is a compilation of the most common mistakes in machine learning and how to avoid them. The book includes examples in the Python programming language. After reading this book, you will be ready to build more robust and trustworthy machine learning models." />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Mistake 17: Wasting unlabeled data | MOST COMMON MISTAKES IN MACHINE LEARNING AND HOW TO AVOID THEM" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/images/cover.jpg" />
  <meta property="og:description" content="This is a compilation of the most common mistakes in machine learning and how to avoid them. The book includes examples in the Python programming language. After reading this book, you will be ready to build more robust and trustworthy machine learning models." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Mistake 17: Wasting unlabeled data | MOST COMMON MISTAKES IN MACHINE LEARNING AND HOW TO AVOID THEM" />
  
  <meta name="twitter:description" content="This is a compilation of the most common mistakes in machine learning and how to avoid them. The book includes examples in the Python programming language. After reading this book, you will be ready to build more robust and trustworthy machine learning models." />
  <meta name="twitter:image" content="/images/cover.jpg" />

<meta name="author" content="Enrique Garcia Ceja" />


<meta name="date" content="2025-09-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="interuservariance.html"/>
<link rel="next" href="appendixInstall.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-X0JKZ260BD"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-X0JKZ260BD');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="#">MOST COMMON MISTAKES IN MACHINE LEARNING AND HOW TO AVOID THEM</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="dedication.html"><a href="dedication.html"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="protecting-the-neural-tree.html"><a href="protecting-the-neural-tree.html"><i class="fa fa-check"></i>Protecting the Neural Tree</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#supplemental-material"><i class="fa fa-check"></i>Supplemental Material</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#conventions"><i class="fa fa-check"></i>Conventions</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#terminology"><i class="fa fa-check"></i>Terminology</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="lackinginsight.html"><a href="lackinginsight.html"><i class="fa fa-check"></i><b>1</b> Lacking insight into the data</a></li>
<li class="chapter" data-level="2" data-path="trainperformance.html"><a href="trainperformance.html"><i class="fa fa-check"></i><b>2</b> Reporting train performance</a></li>
<li class="chapter" data-level="3" data-path="settingseed.html"><a href="settingseed.html"><i class="fa fa-check"></i><b>3</b> Not setting a seed value</a></li>
<li class="chapter" data-level="4" data-path="irrelevantfeatures.html"><a href="irrelevantfeatures.html"><i class="fa fa-check"></i><b>4</b> Including irrelevant features</a></li>
<li class="chapter" data-level="5" data-path="ignoringscales.html"><a href="ignoringscales.html"><i class="fa fa-check"></i><b>5</b> Ignoring differences in scales</a></li>
<li class="chapter" data-level="6" data-path="usingtestset.html"><a href="usingtestset.html"><i class="fa fa-check"></i><b>6</b> Using the test set for fine tunning</a></li>
<li class="chapter" data-level="7" data-path="onlyaccuracy.html"><a href="onlyaccuracy.html"><i class="fa fa-check"></i><b>7</b> Only reporting accuracy</a></li>
<li class="chapter" data-level="8" data-path="notbaseline.html"><a href="notbaseline.html"><i class="fa fa-check"></i><b>8</b> Not comparing against a baseline</a></li>
<li class="chapter" data-level="9" data-path="accountingvariance.html"><a href="accountingvariance.html"><i class="fa fa-check"></i><b>9</b> Not accounting for variance</a></li>
<li class="chapter" data-level="10" data-path="datainjection.html"><a href="datainjection.html"><i class="fa fa-check"></i><b>10</b> Injecting data into the test set</a></li>
<li class="chapter" data-level="11" data-path="notshuffling.html"><a href="notshuffling.html"><i class="fa fa-check"></i><b>11</b> Not shuffling the training data</a></li>
<li class="chapter" data-level="12" data-path="savingresults.html"><a href="savingresults.html"><i class="fa fa-check"></i><b>12</b> Not saving the results</a></li>
<li class="chapter" data-level="13" data-path="notparallelizing.html"><a href="notparallelizing.html"><i class="fa fa-check"></i><b>13</b> Not parallelizing</a></li>
<li class="chapter" data-level="14" data-path="encodingintegers.html"><a href="encodingintegers.html"><i class="fa fa-check"></i><b>14</b> Encoding categories as integers</a></li>
<li class="chapter" data-level="15" data-path="datachanges.html"><a href="datachanges.html"><i class="fa fa-check"></i><b>15</b> Forgetting that data changes over time</a></li>
<li class="chapter" data-level="16" data-path="interuservariance.html"><a href="interuservariance.html"><i class="fa fa-check"></i><b>16</b> Ignoring inter-user variance</a></li>
<li class="chapter" data-level="17" data-path="wastingunlabeled.html"><a href="wastingunlabeled.html"><i class="fa fa-check"></i><b>17</b> Wasting unlabeled data</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixInstall.html"><a href="appendixInstall.html"><i class="fa fa-check"></i><b>A</b> Setup Your Environment</a></li>
<li class="chapter" data-level="B" data-path="appendixDatasets.html"><a href="appendixDatasets.html"><i class="fa fa-check"></i><b>B</b> Datasets</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendixDatasets.html"><a href="appendixDatasets.html#california-housing"><i class="fa fa-check"></i><b>B.1</b> CALIFORNIA-HOUSING</a></li>
<li class="chapter" data-level="B.2" data-path="appendixDatasets.html"><a href="appendixDatasets.html#diagnostic"><i class="fa fa-check"></i><b>B.2</b> DIAGNOSTIC</a></li>
<li class="chapter" data-level="B.3" data-path="appendixDatasets.html"><a href="appendixDatasets.html#digits"><i class="fa fa-check"></i><b>B.3</b> DIGITS</a></li>
<li class="chapter" data-level="B.4" data-path="appendixDatasets.html"><a href="appendixDatasets.html#income"><i class="fa fa-check"></i><b>B.4</b> INCOME</a></li>
<li class="chapter" data-level="B.5" data-path="appendixDatasets.html"><a href="appendixDatasets.html#iris"><i class="fa fa-check"></i><b>B.5</b> IRIS</a></li>
<li class="chapter" data-level="B.6" data-path="appendixDatasets.html"><a href="appendixDatasets.html#wine"><i class="fa fa-check"></i><b>B.6</b> WINE</a></li>
<li class="chapter" data-level="B.7" data-path="appendixDatasets.html"><a href="appendixDatasets.html#wisdm"><i class="fa fa-check"></i><b>B.7</b> WISDM</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="citing-this-book.html"><a href="citing-this-book.html"><i class="fa fa-check"></i>Citing this Book</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MOST COMMON MISTAKES IN MACHINE LEARNING AND HOW TO AVOID THEM</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="wastingunlabeled" class="section level1 hasAnchor" number="17">
<h1><span class="header-section-number">Mistake 17:</span> Wasting unlabeled data<a href="wastingunlabeled.html#wastingunlabeled" class="anchor-section" aria-label="Anchor link to header"></a></h1>

<div class="rmdmistake">
Not using unlabeled data to train a model can lead to a missing opportunity to improve performance.
</div>
<p></br></p>
<p>In many applications it becomes difficult to label data but it is relatively easy to collect unlabeled data. For example, a huge amount of data from internet network traffic is generated every second. However, labeling it as legitimate or malicious may involve human manual inspection. This is where <em>semi-supervised learning (SSL)</em> methods come into play. SSL is ideal when only a small proportion of the data is labeled but still, there are tons of unlabeled examples. With classic supervised learning methods that only rely on fully labeled data, the remaining unlabeled data are discarded (wasted).</p>
<p>SSL algorithms not only try to learn from labeled data but also from unlabeled examples. While there exist many SSL methods <span class="citation">(<a href="#ref-van2020survey">Van Engelen and Hoos 2020</a>)</span>, here, I will focus in one of the simplest ones: <em>self-learning</em>. This algorithm starts with a base classifier and trains it with the available labeled data as usual. Then, the base classifier is used to predict the labels of the unlabeled instances. These are typically called <em>pseudo-labels</em>. Based on some criteria, the best instances along with their pseudo-labels are added to the original training set and the base model is retrained. This process is repeated until some stop condition is met. For example, when the unlabeled data is exhausted or no new instances were added in the last iteration. The criterion for choosing the best instances can be to select the top-<span class="math inline">\(k\)</span> based on their prediction score. Another criterion is to select all instances whose prediction score was above a given threshold.</p>
<p>SSL classifiers depend on a set of assumptions <span class="citation">(<a href="#ref-van2020survey">Van Engelen and Hoos 2020</a>)</span>. One of them is that the unlabeled instances correspond to examples with the same labels and distribution as those contained in the training set. The <em>smoothness assumption</em> states that if two instances are close in the feature space, they should belong to the same class. The <em>low-density assumption</em> says that the decision boundaries of a model should pass through low-density regions and the <em>manifold assumption</em> that instances on the same lower-dimension space should belong to the same class.</p>
<p>To demonstrate the use of self-learning, we will use the <em>DIGITS</em> dataset. First, we load it into memory and split it into train and test sets (50/50). If we print the shape of the train set, we see that it has <span class="math inline">\(898\)</span> instances.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb63-1"><a href="wastingunlabeled.html#cb63-1"></a>data <span class="op">=</span> load_digits()</span>
<span id="cb63-2"><a href="wastingunlabeled.html#cb63-2"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(data.data,</span>
<span id="cb63-3"><a href="wastingunlabeled.html#cb63-3"></a>                                                    data.target,</span>
<span id="cb63-4"><a href="wastingunlabeled.html#cb63-4"></a>                                                    test_size <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span id="cb63-5"><a href="wastingunlabeled.html#cb63-5"></a>                                                    random_state <span class="op">=</span> <span class="dv">1234</span>)</span>
<span id="cb63-6"><a href="wastingunlabeled.html#cb63-6"></a><span class="bu">print</span>(X_train.shape)</span></code></pre></div>
<div class="sourceCode" id="cb64"><pre class="sourceCode python python-output"><code class="sourceCode python"><span id="cb64-1"><a href="wastingunlabeled.html#cb64-1" tabindex="-1"></a><span class="co">#&gt;&gt; (898, 64)</span></span></code></pre></div>
<p>We will simulate that only <span class="math inline">\(100\)</span> instances are labeled and the rest (<span class="math inline">\(798\)</span>) will be unlabeled. To instruct the semi-supervised methods in scikit-learn that an instance is unlabeled, we need to set its label to <span class="math inline">\(-1\)</span>. The following code sets the label of the first <span class="math inline">\(798\)</span> instances to <span class="math inline">\(-1\)</span>. The last instances will be the labeled ones.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb65-1"><a href="wastingunlabeled.html#cb65-1"></a>unlabeled <span class="op">=</span> <span class="dv">798</span> <span class="co"># Number of unlabeled instances.</span></span>
<span id="cb65-2"><a href="wastingunlabeled.html#cb65-2"></a></span>
<span id="cb65-3"><a href="wastingunlabeled.html#cb65-3"></a><span class="co"># Set first n instances as unlabeled.</span></span>
<span id="cb65-4"><a href="wastingunlabeled.html#cb65-4"></a>y_train[:unlabeled] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb65-5"><a href="wastingunlabeled.html#cb65-5"></a></span>
<span id="cb65-6"><a href="wastingunlabeled.html#cb65-6"></a><span class="co"># Check that the first n instances are unlabeled.</span></span>
<span id="cb65-7"><a href="wastingunlabeled.html#cb65-7"></a><span class="bu">print</span>(y_train)</span></code></pre></div>
<div class="sourceCode" id="cb66"><pre class="sourceCode python python-output"><code class="sourceCode python"><span id="cb66-1"><a href="wastingunlabeled.html#cb66-1" tabindex="-1"></a><span class="co">#&gt;&gt; [-1 -1 -1 -1 -1 -1 -1 -1 -1 ... 5  0  9  2  9]</span></span></code></pre></div>
<p>In this example we will use a <code>RandomForest</code> as the base model. Then, we instantiate a <code>SelfTrainingClassifier</code> object that implements the self-learning approach. The first parameter is the base model (a random forest in this case). The <code>threshold</code> specifies the minimum prediction score for an instance to be added to the train set. The <code>verbose</code> parameter specifies if additional information is printed to the console when fitting the model. In this case we set it to <code>True</code> because we want to analyze that information.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb67-1"><a href="wastingunlabeled.html#cb67-1"></a>rf <span class="op">=</span> RandomForestClassifier(n_estimators <span class="op">=</span> <span class="dv">50</span>, random_state <span class="op">=</span> <span class="dv">123</span>)</span>
<span id="cb67-2"><a href="wastingunlabeled.html#cb67-2"></a>ss_model <span class="op">=</span> SelfTrainingClassifier(rf, threshold <span class="op">=</span> <span class="fl">0.95</span>, verbose <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb67-3"><a href="wastingunlabeled.html#cb67-3"></a>ss_model.fit(X_train, y_train)</span></code></pre></div>
<div class="sourceCode" id="cb68"><pre class="sourceCode python python-output"><code class="sourceCode python"><span id="cb68-1"><a href="wastingunlabeled.html#cb68-1" tabindex="-1"></a><span class="co">#&gt;&gt; End of iteration 1, added 1 new labels. </span></span>
<span id="cb68-2"><a href="wastingunlabeled.html#cb68-2" tabindex="-1"></a><span class="co">#&gt;&gt; End of iteration 2, added 1 new labels.</span></span>
<span id="cb68-3"><a href="wastingunlabeled.html#cb68-3" tabindex="-1"></a><span class="co">#&gt;&gt; End of iteration 3, added 2 new labels.</span></span>
<span id="cb68-4"><a href="wastingunlabeled.html#cb68-4" tabindex="-1"></a><span class="co">#&gt;&gt; End of iteration 4, added 3 new labels.</span></span>
<span id="cb68-5"><a href="wastingunlabeled.html#cb68-5" tabindex="-1"></a><span class="co">#&gt;&gt; End of iteration 5, added 5 new labels.</span></span></code></pre></div>
<p>By looking at the output, we can see that in the first iteration <span class="math inline">\(1\)</span> new instance was added to the train set. This instance was the only one that met the threshold criteria specified as an argument. In the fourth iteration, <span class="math inline">\(3\)</span> instances were added. By default, the maximum iterations are set to <span class="math inline">\(10\)</span> but this can be specified with the <code>max_iter</code> parameter. The algorithm stopped at the fifth iteration since no new predictions achieved a score of <span class="math inline">\(0.95\)</span> or more.</p>
<p>Now that the self-training procedure is complete, a performance report can be generated on the test set.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb69-1"><a href="wastingunlabeled.html#cb69-1"></a>y_pred <span class="op">=</span> ss_model.predict(X_test)</span>
<span id="cb69-2"><a href="wastingunlabeled.html#cb69-2"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span></code></pre></div>
<div class="sourceCode" id="cb70"><pre class="sourceCode python python-output"><code class="sourceCode python"><span id="cb70-1"><a href="wastingunlabeled.html#cb70-1" tabindex="-1"></a><span class="co">#&gt;&gt;               precision    recall  f1-score   support</span></span>
<span id="cb70-2"><a href="wastingunlabeled.html#cb70-2" tabindex="-1"></a><span class="co">#&gt;&gt;</span></span>
<span id="cb70-3"><a href="wastingunlabeled.html#cb70-3" tabindex="-1"></a><span class="co">#&gt;&gt;           0       0.93      0.97      0.95        79</span></span>
<span id="cb70-4"><a href="wastingunlabeled.html#cb70-4" tabindex="-1"></a><span class="co">#&gt;&gt;           1       0.78      0.91      0.84        93</span></span>
<span id="cb70-5"><a href="wastingunlabeled.html#cb70-5" tabindex="-1"></a><span class="co">#&gt;&gt;           2       0.85      0.95      0.90        86</span></span>
<span id="cb70-6"><a href="wastingunlabeled.html#cb70-6" tabindex="-1"></a><span class="co">#&gt;&gt;           3       0.90      0.90      0.90        94</span></span>
<span id="cb70-7"><a href="wastingunlabeled.html#cb70-7" tabindex="-1"></a><span class="co">#&gt;&gt;           4       0.97      0.92      0.94       107</span></span>
<span id="cb70-8"><a href="wastingunlabeled.html#cb70-8" tabindex="-1"></a><span class="co">#&gt;&gt;           5       0.87      0.79      0.83       102</span></span>
<span id="cb70-9"><a href="wastingunlabeled.html#cb70-9" tabindex="-1"></a><span class="co">#&gt;&gt;           6       0.95      0.94      0.95        88</span></span>
<span id="cb70-10"><a href="wastingunlabeled.html#cb70-10" tabindex="-1"></a><span class="co">#&gt;&gt;           7       0.89      0.93      0.91        88</span></span>
<span id="cb70-11"><a href="wastingunlabeled.html#cb70-11" tabindex="-1"></a><span class="co">#&gt;&gt;           8       0.98      0.56      0.71        82</span></span>
<span id="cb70-12"><a href="wastingunlabeled.html#cb70-12" tabindex="-1"></a><span class="co">#&gt;&gt;           9       0.70      0.85      0.77        80</span></span>
<span id="cb70-13"><a href="wastingunlabeled.html#cb70-13" tabindex="-1"></a></span>
<span id="cb70-14"><a href="wastingunlabeled.html#cb70-14" tabindex="-1"></a><span class="co">#&gt;&gt;    accuracy                           0.88       899</span></span>
<span id="cb70-15"><a href="wastingunlabeled.html#cb70-15" tabindex="-1"></a><span class="co">#&gt;&gt;   macro avg       0.88      0.87      0.87       899</span></span>
<span id="cb70-16"><a href="wastingunlabeled.html#cb70-16" tabindex="-1"></a><span class="co">#&gt;&gt;weighted avg       0.89      0.88      0.87       899</span></span></code></pre></div>
<p>The overall classification performance using only <span class="math inline">\(100\)</span> instances in the train set was <span class="math inline">\(0.88\)</span> which is not bad. If we follow a traditional supervised learning approach (see accompanying code) which is only able to use the labeled instances, the accuracy drops to <span class="math inline">\(0.85\)</span>.</p>

<div class="rmdcaution">
You should always be cautious when using semi-supervised learning methods. If the underlying model generates poor predictions or the required assumptions are not met, the errors can propagate causing a drop in performance.
</div>
<p>While in this example there was a performance gain when using a semi-supervised approach, this also depends on the chosen parameters. For example, if the decision threshold is decreased, erroneous labels will make its way into the train set and as a consequence, the performance of the model will decrease.</p>
</br>

<div class="rmdsolution">
Semi-supervised learning methods can be used to learn from unlabeled examples in settings where labeled data is scarse. Scikit-learn implements some approaches like <code>SelfTrainingClassifier</code> and <code>LabelPropagation</code> for SSL classification tasks.
</div>

</div>



<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-van2020survey" class="csl-entry">
Van Engelen, Jesper E, and Holger H Hoos. 2020. <span>“A Survey on Semi-Supervised Learning.”</span> <em>Machine Learning</em> 109 (2): 373–440.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interuservariance.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendixInstall.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
