[["index.html", "MOST COMMON MISTAKES IN MACHINE LEARNING AND HOW TO AVOID THEM (with examples in Python) Welcome About the Author", " MOST COMMON MISTAKES IN MACHINE LEARNING AND HOW TO AVOID THEM (with examples in Python) Enrique Garcia Ceja 2025-06-04 Welcome This is the free html version of the book. The pdf version can be purchased (free for a limited period of time!) at Leanpub. This book is a compilation of the most common mistakes when building machine learning models. I have gathered this list from mistakes I typically find when grading assignments, supervising graduate students, reading blog posts, looking at the accompanying code of published papers, and of course, from my own experience making those mistakes. Understanding and being aware of those common mistakes will allow you not only to avoid them, but to build better machine learning systems and less prone to errors. This book includes examples in Python. Some examples of mistakes that you will find in this book include: Not understanding the data Including irrelevant variables Data injection Assuming all users behave the same Wasting unlabeled data and much more! The accompanying source code for all examples is available at https://github.com/enriquegit/ml-mistakes-code The front cover captures a representation of the majestic Neural Tree being attacked by the so called neuruptors. Fortunately, the Science-Fish-Team is here to protect the integrity and well-being of the tree. About the Author My name is Enrique and I am a researcher/professor at Tecnologico de Monterrey University. Previously, I worked as a Research Scientist at Optimeering and at SINTEF Research Center, Norway. I did a postdoc at the University of Oslo and obtained my PhD from Tecnologico de Monterrey University, Mexico. I also worked as a software engineer at Huawei. Contact me: You can drop me a message and/or rate this book by filling this form. website: http://www.enriquegc.com This is a live document and I will keep updating it and adding new content. Chapters’ order may change. Copyright © 2025 Enrique Garcia Ceja All rights reserved. No part of this book may be reproduced or used in any manner without the prior written permission of the copyright owner, except for the use of brief quotations. To request permissions, contact the author. "],["dedication.html", "Dedication", " Dedication To My Family, who have put up with me despite my continuous mistakes. "],["protecting-the-neural-tree.html", "Protecting the Neural Tree", " Protecting the Neural Tree The Neural Tree is a type of binary tree where its nodes are composed of neurons. It has lived in the depths of the sea for millions of years. It provides knowledge and answers to the most intricate questions of life. Its roots continuously absorb knowledge from the seafloor and its long and strong stems provide shelter to many marine organisms. Everything was going on with peace and tranquility until one day a group of neuruptors arrived and started to attack the Neural Tree. Neuruptors are poisonous-plasma-made creatures whose only purpose is to disrupt the neural functions of the tree. They can shoot plasma flares that adhere to the tree causing memory loss and confusion and thus, making it generate erroneous predictions. Dr. Nepturo was swimming as usual in the area when suddenly, he spotted one of the neuruptors approaching. As the leader of the Science-Fish-Team (SFT), he called the four agents that act under his guidance to come to the rescue. The first to arrive was Sotaco who is the smallest of the team but at the same time, the fastest. He arrived piloting the aquadome and with one shot, he destroyed one of the neuruptors. Second to arrive, were the twins Lunia and Corsatu. Lunia emits an electric shock to stop one of the creatures while Corsatu uses his bionic fin to repel the plasma shot at the tree. He lost his fin in one of the many battles he has fought and built himself a robotic one. Finally, Juptano arrived at the scene. Juptano is the strongest of the team. After apologizing for being late (he was finishing his breakfast), he quickly swam towards the excavator that was breaking the roots of the tree. The battle was becoming fierce and exhausting and more neuruptors started to arrive. How much longer will the Science-Fish-Team be able to sustain the battle? "],["preface.html", "Preface", " Preface This book is a compilation of the most common mistakes when building machine learning systems with examples in Python. It is assumed that the reader already has some experience with machine learning and with the scikit-learn package. This book can also be used as a complementary source when learning about machine learning for the first time. After reading this book, you will be ready to build more robust and trustworthy machine learning models. Supplemental Material Supplemental material consists of the examples’ code and datasets. The source code for the examples can be downloaded from https://github.com/enriquegit/ml-mistakes-code. Instructions on how to set up the code are in Appendix A. A reference for all the utilized datasets is in Appendix B. Conventions DATASET names are written in uppercase italics. Functions are referred to by their name followed by parenthesis and omitting their arguments, for example: myFunction(). Class labels are written in italics and between single quotes: ‘label1’. The following icons are used to provide additional contextual information: Provides a brief summary of the mistake. Important information to consider. A summary of how a mistake can be avoided. Acknowledgements I want to thank all my fish for their company while writing this book. Figure 0.1: The real Science-Fish-Team. "],["introduction.html", "Introduction", " Introduction Building error-free machine-learning-based systems is crucial, specially, nowadays that these systems are everywhere. From room-size servers stacked into huge buildings, to coin-size deep learning accelerators, these devices run machine learning algorithms that support daily critical operations in finance, logistics, aerospace, health, autonomous trucks/vehicles, and so on. Thus, it is our task to make sure those systems are robust, reliable, and trustworthy. During the process of building a machine learning model, there are many points in which errors can make their way in –from data data collection, to deployment. The environment (and thus the data) is continuously changing. As such, one needs to be vigilant and keep monitoring the system to protect it against potential errors. This book showcases the most common mistakes when building machine learning models during different phases. Each chapter focuses on a single mistake. For the sake of keeping the examples simple and focusing on the mistake at hand, some may incur in other mistakes. Every chapter demonstrates the corresponding mistake with code snippets and ways to avoid/correct the error. Library import statements and minor details may be omitted to keep the code simple however, the full source code of the examples is available. Before delving into the mistakes, the following subsection introduces the terminology that will be used. So, I recommend you to take a look and familiarize yourself with the terms. With the help of the Science-Fish-Team, let’s begin our journey against the neuruptors!!! Terminology In this section, I will introduce the terminology that we will use for the rest of the book. Most of the time, datasets are stored as tables or in Python terminology, pandas data frames or numpy arrays. Figure 0.2 shows the classic IRIS dataset (Fisher 1936) stored in a pandas data frame. Figure 0.2: First rows of the IRIS dataset. The columns represent variables (features) and the rows represent examples also known as instances or data points. In this table, there are \\(5\\) variables sepal length, sepal width, petal length, petal width and the class. Each row represents a specific plant. In machine learning terminology, rows are more commonly called instances whereas in statistics they are often called data points or observations. Here, I will use those terms interchangeably. Figure 0.3 shows the same data frame. Suppose that we are interested in predicting the class based on the other variables. In machine learning terminology, the variable of interest (the one that depends on the others) is called the class or label for classification problems. For regression, it is often referred to as y. In statistics, it is more commonly known as the response, dependent, or y variable, for both, classification and regression. Figure 0.3: First rows of theIRIS dataset. The first four columns are the features and the last column is the class. In machine learning terminology, the variables used to predict the class are called features or attributes. In statistics, they are typically referred as predictors, independent variables, or X. The term feature vector is used often in machine learning. A feature vector is an array containing the features of a given instance. For example, the feature vector of the first instance in Figure 0.3 is \\([5.1,3.5,1.4,0.2]\\). Regarding algorithms, the term model will be used to refer to classifiers or regression models. From the context, it should be clear which one it refers to, or if both. Now that we are armed with this terminology, let’s begin fighting those neuruptors!!! References Fisher, Ronald A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7 (2): 179–88. "],["notunderstandingdata.html", "Mistake 1: Not understanding the data", " Mistake 1: Not understanding the data Not understanding the details of the dataset before processing it and training models. With today’s newest software tools, it is straightforward to load a dataset and start training machine learning models. However, this can lead to erroneous models or incorrect conclusions if performed without having a good understanding of the data. Before delving into writing code, it is advisable to carefully read the dataset documentation and if possible, to talk with the authors in case you did not take part in the data collection process. By doing so, you may find out that some of the data are not relevant and thus, should not be included in your analysis. On the other hand, you may learn that there are important details that if not taken into account, could completely change the results. After having a clear high-level picture of the dataset, it is a good idea to perform an exploratory data analysis (EDA). This will allow you to better understand the structure of the data and find possible errors. An EDA consists of a set of tools, methods, and visualizations that you can employ to inspect your data. For example, boxplots, correlation plots, histograms, scatter plots, and so on. It is also a good idea to check the variables’ types (numeric, boolean, string, etc.). At this stage you can also check for missing values. If most of the values of a variable are missing or it has a constant value, then you may want to discard it. Another source of error is having duplicate rows and outliers. An EDA will allow you to find those types of errors. There are tools that can help you with the EDA. For example, the ydata-profiling package (ydata-profiling team 2025) generates a report with summary statistics and plots for a given pandas data frame with a few lines of code. The following example loads the IRIS dataset and generates an EDA report. from sklearn.datasets import load_iris data = load_iris(as_frame = True).frame from ydata_profiling import ProfileReport profile = ProfileReport(data, title=&quot;My Report&quot;) profile Figure 1.1 shows the generated overall statistics of the dataset. Here, you can see that there was one duplicate row, four numeric and one categorical variables, and so on. Figure 1.1: Statistics of the data frame. The report also generates statistics for each individual variable. Figure 1.2 shows the report for sepal length along with its histogram. The report also generates information about interactions between variables in the form of scatter plots and correlation plots. Figure 1.2: Histogram of one of the variables. While these types of tools can save you a lot of time you may still need to perform some more detailed EDA depending on your needs. Before processing a dataset it is always a good idea to understand the details such as the variables, the context, and so on. It is also a good practice to perform an exploratory data analysis to identify noise, missing values, errors, etc. References ydata-profiling team. 2025. Ydata-Profiling Python Package. https://github.com/ydataai/ydata-profiling. "],["trainperformance.html", "Mistake 2: Reporting train performance", " Mistake 2: Reporting train performance Reporting train performance metrics instead of test performance. The ultimate goal of computing performance metrics is to estimate the generalization performance of your model. That is, you want to know how it is going to behave when fed with new data. This is the main reason for dividing the dataset into train and test sets. Thus, you typically train a model using the train set and evaluate its performance with the test set. The test set performance is what you report most of the time. However, it is easy to mix the sets when coding your program. Take for example, the following code snippet. Here, a decision tree is fitted with the WINE dataset. Before the fitting, the data is split into train and test sets. from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.datasets import load_wine # Load the wine dataset. data = load_wine() # Divide into train and test sets. X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size = 0.5, random_state = 123) tree = DecisionTreeClassifier(random_state = 123) tree.fit(X_train, y_train) predictions = tree.predict(X_train) print(f&quot;Decision Tree accuracy: {accuracy_score(y_train, predictions):.3f}&quot;) #&gt;&gt; Decision Tree accuracy: 1.000 What is wrong with the previous code? Actually, there is nothing wrong with it. It runs without any errors and it prints the accuracy (\\(1.0\\)) as expected. However, there is something suspicious here. The accuracy is perfect. When your results look so good, it may be an indication that there is something going on. If you take a closer look at lines 16 and 17 you will notice that the performance (accuracy) is being computed on the train set. If your aim is to estimate the generalization performance of your model then, you should instead compute the performance on the test set. Even though you already know this, it is common to incur in this type of error when copying and pasting code. If what you are really interested in is the generalization performance, then, lines 16-17 should be changed to: predictions = tree.predict(X_test) print(f&quot;Decision Tree accuracy: {accuracy_score(y_test, predictions):.3f}&quot;) #&gt;&gt; Decision Tree accuracy: 0.865 In this case the accuracy was much lower (\\(0.865\\)) but represents a better estimate of what you would expect when you deploy your system into production. This does not mean that you should never compute the training performance. In fact, it is also very useful to have this information at hand. For example, for diagnosing purposes when analyzing overfitting. If your results look suspiciously good, it is worth to double check your code to see if you are not reporting the train performance instead of test performance. "],["settingseed.html", "Mistake 3: Not setting a seed value", " Mistake 3: Not setting a seed value Not setting a seed value when using functions that rely on random number generators can make it difficult to reproduce the results. When working in machine learning, most of the time you will be dealing with non-deterministic functions. Non-deterministic means that some randomness is involved in the process, and typically, this happens when making a call (directly or indirectly) to a random number generator. For example, when splitting the data into train and test sets, you do so with the train_test_split() function from scikit-learn. The following code snippet loads the WINE dataset and splits it into a train and a test set. Then, it prints the first ten classes in the train set. from sklearn.model_selection import train_test_split from sklearn.datasets import load_wine # Load the wine dataset. data = load_wine() # Divide into train and test sets. X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.5) # Print the first classes in the train set. print(y_train[0:10]) #&gt;&gt; [1 0 1 1 0 2 1 1 1 2] If you run the previous code multiple times, you will get different results every time. And in fact, this is the behavior you are expecting since the data should be split randomly. At this point you may be wondering: Where is the error? or Why is this being considered as a mistake? The reason I categorize this as a mistake is because in some circumstances, including when you publish your results or share your code with someone else, it will be difficult to get the same result again and again. But, isn’t this contradictory? On one hand, you want to randomly split the data, but on the other hand, you want to get the same results every time. The answer is that since random number generators are not truly random but pseudo-random, then, you can achieve both objectives. You can simulate a random process and at the same time make it reproducible. To do so, you can set a fixed seed value for the random generator. Thus, every time you call a function based on a random number generator it will produce the same result with respect to the initial seed value. In scikit-learn, you can set the seed value for various of its functions through the random_state parameter. The following code adds the random_state = 123 parameter to the train_test_split() function. This parameter accepts an integer number. from sklearn.model_selection import train_test_split from sklearn.datasets import load_wine # Load the wine dataset. data = load_wine() # Divide into train and test sets. X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.5, random_state = 123) # Print the first classes in the train set. print(y_train[0:10]) #&gt;&gt; 2 2 2 1 0 0 1 0 1 1 Now every time you run the previous code it will generate the same result based on the value of the random_state parameter. If you change its value, a different train/test split will be generated but it is guaranteed that you will get the same result if you use the same integer value. If you are training a model that relies on random number generators, then, you should set the random_state parameter as well. For example, the RandomForestClassifier picks random features to fit each of its trees. The following code crates a random forest model and sets its random state. from sklearn.ensemble import RandomForestClassifier # Instantiate a RandomForestClassifier and set its random state. rf = RandomForestClassifier(n_estimators=50, random_state=123) Be aware that sometimes even when setting the random_state parameter, you may get different results. This can happen if you run the same code but using different library versions. It has been reported that some scikit-learn functions produce different results across versions (Guigui14460 2021). I have also experienced some incongruencies with the RandomForestClassifier when running it with different versions of scikit-learn. Apart from setting the random seed, you should also document which version of Python and libraries were used in your program. In scikit-learn you can set the random_state parameter in several functions to make sure you always get the same results. References Guigui14460. 2021. Differences in Scores Between Two Different Versions. https://github.com/scikit-learn/scikit-learn/issues/20042. "],["irrelevantfeatures.html", "Mistake 4: Including irrelevant features", " Mistake 4: Including irrelevant features Training your model with irrelevant features. When training a model, it is recommended to avoid using irrelevant features. Adding such features can degrade the model’s performance in terms of time, interpretabilty, and performance. Irrelevant features are the ones that do not provide any information, introduce noise, increase overfitting, and so on. Examples of such features include: Row numbers. Sometimes, datasets come with a column specifying row numbers. These numbers do not add information and will likely cause overfitting if included in the model. Unique identifiers. Similar to row numbers, unique identifiers like usernames, e-mail addresses, ids, etc. are specific to each row and thus, will degrade the model’s performance. Timestamps. If rows are assumed to be independent, then, including timestamp information can cause severe overfitting. Constant values. This happens when the majority (or all) of the instances have the same value for a given feature. For example, a column having all zeros. In other words, there is low variance in the values. Features with many missing values. Having many missing values will not add significant information to the model. Non related information. Some features may include unrelated information to the problem at hand. For example, if trying to predict hand gestures from videos, hair color will not provide any useful information. Highly correlated variables. Variables with high correlation may be redundant and can add computational costs. Of course, there are exceptions. For instance, timestamps are important when modeling timeseries data, thus, careful consideration should be made before discarding features depending on the problem. As an example, we will use the WISDM dataset (Kwapisz, Weiss, and Moore 2010). The dataset has \\(43\\) features extracted from a cellphone accelerometer while several participants performed physical activities. Suppose that you want to predict the type of activity based on the acceleromter’s features. The following code reads the data. Missing values are denoted with the ‘?’ symbol. Thus, the na_values = \"?\" parameter is set to let know pandas that question marks should be marked as missing values (line \\(3\\)). Apart from that, the columns YAVG and ZAVG have erroneous values that look like “?0.01”. Those values will be set as missing values as well (lines \\(6\\) and \\(7\\)). Figure 4.1 shows the first rows and columns of the data frame. import pandas as pd df = pd.read_csv(&quot;data/wisdm/WISDM_ar_v1.1_transformed.csv&quot;, na_values = &quot;?&quot;) # Mark values including a &#39;?&#39; such as &#39;?0.01&#39; as NaN. df[&#39;YAVG&#39;] = pd.to_numeric(df[&#39;YAVG&#39;], errors=&#39;coerce&#39;) df[&#39;ZAVG&#39;] = pd.to_numeric(df[&#39;ZAVG&#39;], errors=&#39;coerce&#39;) df.head() Figure 4.1: WISDM dataset. The first thing to note is the UNIQUE_ID and user columns. The UNIQUE_ID indicates the row number and useris the user id. This information is irrelevant for the problem so the columns can be discarded. # Remove columns &#39;UNIQUE_ID&#39; and &#39;user&#39; df = df.drop(columns=[&#39;UNIQUE_ID&#39;, &#39;user&#39;]) An EDA analysis (see Mistake 1) can also be used to spot irrelevant variables. The report using the ydata-profiling package generates some alerts (see Figure 4.2). Figure 4.2: WISDM alerts. The first alert shows that the XAVG column has a constant value of \\(0\\). The next alerts are about highly correlated variables. scikit-learn provides a way for identifying variables with low variance. The next code stores the features in variable X and uses a VarianceThreshold object to identify such variables. The user can specify a threshold (\\(0.0\\) in this example). # Find features with low variance. from sklearn.feature_selection import VarianceThreshold X = df.drop(&#39;class&#39;, axis=1) feature_names = X.columns selector = VarianceThreshold(threshold = 0.0) X = selector.fit_transform(X) # Print names of removed features mask = selector.get_support() removed_features = feature_names[~mask] print(&quot;Removed features:&quot;, list(removed_features)) #&gt;&gt; Removed features: [&#39;XAVG&#39;] In this case, the XAVG variable was identified as having zero variance and deleted. You can use several tools to identify irrelevant features such as correlation plots, variance thresholds, histograms, and so on. An exploratory data analyis will help you identify irrelevant variabls (see for example, Mistake 1). References Kwapisz, Jennifer R., Gary M. Weiss, and Samuel A. Moore. 2010. “Activity Recognition Using Cell Phone Accelerometers.” In Proceedings of the Fourth International Workshop on Knowledge Discovery from Sensor Data (at KDD-10), Washington DC. "],["ignoringscales.html", "Mistake 5: Ignoring differences in scales", " Mistake 5: Ignoring differences in scales Ignoring differences in features’ scales. As previously mentioned (see Mistake 1), it is always a good idea to perform an exploratory analysis of your data. At this point, you may realize that the features have different scales. For example, let’s load the DIAGNOSTIC dataset and compute some summary statistics with the describe() function. Figure 5.1 shows a summary of the data frame. from sklearn.datasets import load_breast_cancer data = load_breast_cancer(as_frame=True) data.frame.describe() Figure 5.1: Summary statistics of the DIAGNOSTIC dataset. Here, we can see that the max value of mean concavity is \\(0.42\\) and the max value of mean area is \\(2501\\). The later is much larger (in the range of thousands). These differences can have a huge impact in many machine learning models. Specially on those based in distances such as KNN. Suppose we want to compute the distance between two feature vectors \\(v_1=[0.21, 2034]\\) and \\(v_2=[0.19, 1327]\\) using the Euclidean distance: \\[\\begin{equation} d = \\sqrt{(0.21 - 0.19)^2 + (2034 - 1327)^2} \\tag{5.1} \\end{equation}\\] It is easy to see that the second feature is dominant, making the first one almost irrelevant. One way to deal with this is by normalizing the features so they fall into the same interval, for example \\([0-1]\\). The following formula can be used to normalize a variable \\(X\\): \\[\\begin{equation} z_i = \\frac{X_i - min(X)}{max(X)-min(X)} \\tag{5.2} \\end{equation}\\] where \\(z_i\\) is the normalized \\(i^{th}\\) value and \\(max(X)\\) and \\(min(X)\\) are the maximum and minimum values of \\(X\\). Scikit-learn has the MinMaxScaler() function that does that transformation. The following code split the data into train and test sets. Then, it fits the scaler using the train data and transforms it (line \\(10\\)). Finally, the test set is transformed as well. Figure 5.2 the summary statistics of the train set after the transformation. from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler X = data.data y = data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 123) scaler = MinMaxScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns) X_train_scaled_df.describe() Figure 5.2: Summary statistics of the DIAGNOSTIC dataset after normalization. Now, the minimum and maximum values of all features are \\(0\\) and \\(1\\), respectively. Note that the scaler was fitted using only the train data. That is, the parameters (max, min) were learned from the train set and then applied to the test set. This separation is done to avoid leaking training parameters into the test set (see Mistake 10). It is worth mentioning that not all models are affected by scale differences. For example, decision trees are immune to differences in scales across features. Scaling the features can reduce the impact of different ranges across them. Scikit-learn implements several scalers including MinMaxScaler, StandardScaler, MaxAbsScaler, etc. "],["usingtestset.html", "Mistake 6: Using the test set for fine tunning", " Mistake 6: Using the test set for fine tunning Using the test set for fine tunning your models and/or preprocessing methods. Most machine learning models require some hyper-parameter tuning. For example, KNN requires to specify the number of nearest neighbors \\(k\\) and the distance function (Euclidean, Manhattan, etc.). In decision trees, you need to specify the feature importance function, maximum tree depth, etc. For ensembles, you need to specify the number of models, and so on. Parameter tuning can also occur when preprocessing the data. For example, when applying a moving average filter to timeseries data, one needs to define the window size. Finding the best hyper-parameters requires trying different values either by hand, or by using optimization procedures such as grid search, genetic algorithms, Bayesian optimization, to name a few. One of the most common mistakes occurs when using the test set to optimize hyper-parameters. Figure 6.1 exemplifies this scenario. Here, the best \\(k\\) for KNN is looked for. The model was fitted using a train set. Then, different values of \\(k\\) (1-3) are evaluated using the test set. In this case, the model is being overfitted to the test set. Figure 6.1: Overfitting a model to the test set. The following code snippet illustrates this same scenario when building a KNN classifier for the WISDM dataset. X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.5, random_state = 123) scaler = MinMaxScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) for i in range(5): knn = KNeighborsClassifier(n_neighbors = i+1) knn.fit(X_train_scaled, y_train) y_pred = knn.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) print(f&quot;k={i+1}&quot; f&quot; accuracy {accuracy:.2f}&quot;) #&gt;&gt; k=1 accuracy 0.83 #&gt;&gt; k=2 accuracy 0.79 #&gt;&gt; k=3 accuracy 0.81 #&gt;&gt; k=4 accuracy 0.80 #&gt;&gt; k=5 accuracy 0.80 The problem with this approach is that the value of \\(k\\) is being chosen specifically for this test set. Thus, the model’s performance is likely being overestimated when reporting the accuracy with the best selected \\(k\\). The correct way of evaluating different hyper-parameters is to do so using an independent set of data. Typically, this one is called the validation set. Instead of dividing the data into two subsets, it should be split into three subsets: train, validation, and test sets. The model is trained with the train set. Then, hyper-parameter optimization is performed using the validation set. Once the best hyper-parameters have been found (\\(k\\) in this example), the final model is tested only once using the test set (and these are the performace measures to be reported). Figure 6.2 shows this procedure. Figure 6.2: Finding the best hyper-parameters using an independent validation set. In code, this would look like this: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 123) # Split the test set into two equal-size sets. X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size = 0.5, random_state = 123) scaler = MinMaxScaler() X_train_scaled = scaler.fit_transform(X_train) X_val_scaled = scaler.transform(X_val) X_test_scaled = scaler.transform(X_test) for i in range(5): knn = KNeighborsClassifier(n_neighbors = i+1) knn.fit(X_train_scaled, y_train) y_pred = knn.predict(X_val_scaled) accuracy = accuracy_score(y_val, y_pred) print(f&quot;k={i+1}&quot; f&quot; accuracy {accuracy:.2f}&quot;) #&gt;&gt; k=1 accuracy 0.83 #&gt;&gt; k=2 accuracy 0.80 #&gt;&gt; k=3 accuracy 0.81 #&gt;&gt; k=4 accuracy 0.80 #&gt;&gt; k=5 accuracy 0.80 Here, half of the test set used as the validation set which is used to find the best \\(k\\) (\\(1\\) in this case). Finally a KNN with \\(k=1\\) is evaluated on the test set, and this is what is reported. best_k = 1 knn = KNeighborsClassifier(n_neighbors = best_k) knn.fit(X_train_scaled, y_train) y_pred = knn.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) print(f&quot; test set accuracy {accuracy:.2f}&quot;) #&gt;&gt; test set accuracy 0.83 When performing hyper-parameter tuning, use an independent validation set. "],["onlyaccuracy.html", "Mistake 7: Only reporting accuracy", " Mistake 7: Only reporting accuracy Only reporting accuracy when assessing the performance of a model. The main purpose when assessing the performance of a model is to understand its generalization capabilities. This evaluation is done mainly through the use of performance metrics (accuracy, recall, precision, etc.). Each metric captures different aspects of the problem at hand. However, sometimes only accuracy is reported which can lead to misleading interpretations. Let’s say your test set has \\(99\\) positive instances and \\(1\\) negative instance. A simple model that always predicts positive regardless of the input will have an accuracy of \\(99\\%\\). However, it will never be able to detect the negative class which is the one you may be the most interested in. By only computing the accuracy, you may think that the model is very good. However, when looking at the recall, it will be \\(0\\%\\) for the negative class. Computing other metrics besides accuracy not only gives you a more complete view of the behavior of the model but also allows you to identify possible issues in the data (e.g., imbalanced classes). The following code shows the use of classification_report() which computes several metrics for each class (\\(0\\), \\(1\\), \\(2\\)). # Load the wine dataset. data = load_wine() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.5, random_state = 123) dt = DecisionTreeClassifier(random_state =123) dt.fit(X_train, y_train) y_pred = dt.predict(X_test) print(classification_report(y_test, y_pred)) #&gt;&gt; precision recall f1-score support #&gt;&gt; #&gt;&gt; 0 0.83 0.97 0.89 30 #&gt;&gt; 1 0.88 0.77 0.82 30 #&gt;&gt; 2 0.89 0.86 0.88 29 #&gt;&gt; #&gt;&gt; accuracy 0.87 89 #&gt;&gt; macro avg 0.87 0.87 0.86 89 #&gt;&gt; weighted avg 0.87 0.87 0.86 89 Computing several metrics allows you to have a better understading of the models performance. "],["notbaseline.html", "Mistake 8: Not comparing against a baseline", " Mistake 8: Not comparing against a baseline Not comparing your models against a baseline. Sometimes you may be tempted to try to solve a problem with a complex machine learning model. You proceed to train and evaluate your complex model and obtain a high performance estimate. So you decide to deploy the model into production. Later, you realize that the expected benefits are not materializing (increased sales, waste reduction, click rates, etc.). What could have gone wrong? It is a common mistake to assume that a good performance estimate (accuracy, recall, etc.) of your model means that the solution to the actual problem will be efficient as well. However, models learn from the data, not from the problem itself. If the data does not characterize the problem well, then, it is likely that the model will not be useful to solve that problem (even if the performance metrics look great). There can be underlying problems with the data that can cause a model to produce good predictions but not useful at all. Examples of such problems include class imbalance and/or having features with no information relevant to the problem. One way to spot such problems is to compare your complex models against base line models. If a simple baseline model performs similar to a complex one, it may be an indication that the later is not learning patterns from the data. It also happens that a problem may be so difficult (e.g., forecasting), that performing slightly better than random is a huge achievement. A baseline model not necessarily needs to be machine-learning-based. It could be something as simple as predicting the mean (for regression), or using a manually set threshold in one of the features to decide the label. Baseline models are also called dummy models. Some dummy models for classification include: Most-frequent-class. This model always predicts the most frequent class. So it does not even need to analyze the features. Uniform predictions. This model predicts classes at random with equal probability. Prior This one predicts a class based on its prior probability. For regression, a baseline model can predict the mean value. In the case of timeseries data, the prediction at time \\(t_i\\) can be the value at \\(t_{i-1}\\), or the average across the previous \\(n\\) values. For the following example, a synthetic dataset with two classes is created. It consists of \\(1000\\) rows and \\(10\\) features. The dataset is forced to be imbalanced with weights=[0.95, 0.05]. X, y = make_classification(n_samples = 1000, n_features = 10, n_redundant = 0, n_classes = 2, weights=[0.95, 0.05], random_state = 123) Next, a neural network (Multi-Layer Perceptron MLP) is trained on the synthetic data. nn = MLPClassifier(hidden_layer_sizes=(5, 2), max_iter = 1000, random_state = 98) nn.fit(X_train, y_train) y_pred = nn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f&quot; Neural Network accuracy {accuracy:.2f}&quot;) #&gt;&gt; Neural Network accuracy 0.95 The final accuracy is \\(0.95\\), which looks great. However, you are not convinced about the results and you decide to train a baseline model. Sci-kit learn provides a DummyClassifier and DummyRegressor. The following code fits a most-frequent DummyClassifier. The strategy argument specifies the type of dummy model: “most_frequent”, “prior”, “stratified”, “uniform”, or “constant”. from sklearn.dummy import DummyClassifier dummy = DummyClassifier(strategy=&#39;most_frequent&#39;, random_state = 123) dummy.fit(X_train, y_train) y_pred = dummy.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f&quot; Dummy classifier (most frequent) accuracy {accuracy:.2f}&quot;) #&gt;&gt; Dummy classifier (most frequent) accuracy 0.95 To our surprise, a most-frequent model performed the same as the neural network. This means that even though the accuracy seems high, the neural network is not providing any real benefit. In this case, the reason being that the classes are not balanced. dummy = DummyClassifier(strategy=&#39;uniform&#39;, random_state = 123) #&gt;&gt; Dummy classifier (uniform) accuracy 0.48 The accuracy of the uniform model was \\(0.48\\). If our neural network had an accuracy of \\(\\approx 0.48\\) (which was not the case) it would had been an indication that the model was not learning. You can use the classes DummyClassifier() and DummyRegressor() to create simple baselines to compare your models. "],["accountingvariance.html", "Mistake 9: Not accounting for variance", " Mistake 9: Not accounting for variance Not accounting for variance when comparing different models can lead to wrong conclusions. Before deploying a model into production, you want to be sure you are using the best model for your given use case. In order to find this best model, you may want to compare between different alternatives. To demonstrate this idea, I will use the DIGITS dataset which consists of \\(8x8\\) images flattened into a feature vector of length \\(64\\). Each feature consists of an integer between \\(0-16\\) representing the pixel intensity. Figure 9.1 shows the first digit with class ‘0’. Figure 9.1: Digit 0. Let’s compare a DecisionTreeClassifier versus a GaussianNB to decide which one is better for digits classification. For the sake of simplicity, I will only use accuracy as the performance metric. import numpy as np from sklearn.datasets import load_digits data = load_digits() from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size = 0.5, random_state = 123) from sklearn.tree import DecisionTreeClassifier from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score tree = DecisionTreeClassifier(random_state=1234) bayes = GaussianNB() tree.fit(X_train, y_train) bayes.fit(X_train, y_train) predictions_tree = tree.predict(X_test) predictions_bayes = bayes.predict(X_test) print(f&quot;Decision Tree accuracy: {accuracy_score(y_test, predictions_tree):.3f}&quot;) print(f&quot;GaussianNB accuracy: {accuracy_score(y_test, predictions_bayes):.3f}&quot;) #&gt;&gt; Decision Tree accuracy: 0.844 #&gt;&gt; GaussianNB accuracy: 0.778 According to those results you may conclude that DecisionTreeClassifier is the best choice. However, it may be the case that if the initial conditions change a little bit (e.g., a slightly different training set), you could get completely different results. And in fact, if you change random_state=1234 to random_state=123 in train_test_split() this time GaussianNB performs better. This is called variance. That is, the results vary from run to run. Then, how can you decide which model is better in the long run? One way is to repeat the experiment several times and select the model that performs better on average. Every time you run the experiment, you randomly partition the train and test sets. In this way, every experiment will have a slightly different train and test sets. This procedure is called Monte Carlo cross-validation. The following code runs \\(500\\) iterations and computes the average accuracy. n = 500 accuracy_tree = [] accuracy_bayes = [] for i in range(n): X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size = 0.5, random_state = 123 + i) tree = DecisionTreeClassifier(random_state=123) bayes = GaussianNB() tree.fit(X_train, y_train) bayes.fit(X_train, y_train) predictions_tree = tree.predict(X_test) predictions_bayes = bayes.predict(X_test) accuracy_tree.append(accuracy_score(y_test, predictions_tree)) accuracy_bayes.append(accuracy_score(y_test, predictions_bayes)) print(f&quot;Decision Tree accuracy: {np.mean(accuracy_tree):.3f}&quot;) print(f&quot;GaussianNB accuracy: {np.mean(accuracy_bayes):.3f}&quot;) #&gt;&gt; Decision Tree accuracy: 0.829 #&gt;&gt; GaussianNB accuracy: 0.837 Here, we can see that GaussianNB is better on average, contrary to what we thought by only running the experiment one iteration. Note that it is important to change the random state in train_test_split() in each iteration (random_state = 123 + i). Otherwise the train and test sets will be the same. Figure 9.2 shows a histogram of the accuracies. Here, we can see that there is some overlap between the two models but on average, GaussianNB is better. Figure 9.2: Histogram of accuracy. The difference between the models’ accuracies can be further validated with a paired t-test. from scipy.stats import ttest_rel t_stat, p_value = ttest_rel(accuracy_tree, accuracy_bayes) print(f&quot;p-value: {p_value:.10f}&quot;) #&gt;&gt; p-value: 0.0000000054 In this example we used Monte Carlo cross-validation to robustly compare two models. However, you should also account for variance when tuning a model’s hyper-parameters, choosing pre-processing methods, data transformations, etc. To account for variance in your results, you can use Monte Carlo cross-validation. This will allow you to generate more confident results and stronger conclusions. "],["datainjection.html", "Mistake 10: Injecting data into the test set", " Mistake 10: Injecting data into the test set Injecting training data into the test set. Data injection is one of the most common mistakes in machine learning and it materializes in many different ways. When assessing the generalization performance of a model, an independent test set is used to compute performance metrics like accuracy, precision, f1-score, etc. The keyword here is: independent. This means that the trained model should not have any information about this independent set. However, there are many ways in which we can inadvertently violate this assumption by ‘injecting’ information into the model that it should not have at test time. This typically happens when applying preprocessing procedures like normalization or class balancing methods. To demonstrate this, we will import the IRIS dataset and train a KNN classifier. First we load the dataset and print some summary statistics. Figure 10.1 shows the output of the describe() function. from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import accuracy_score from sklearn.datasets import load_iris data = load_iris(as_frame=True) data.frame.describe() Figure 10.1: IRIS dataset summary. By looking at the statistics, we notice that the columns have different ranges so we decide to normalize the features, split the data, and train the classifier. X = data.data y = data.target scaler = MinMaxScaler() X_scaled = scaler.fit_transform(X) X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.5, random_state = 123) knn = KNeighborsClassifier(n_neighbors = 3) knn.fit(X_train, y_train) y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f&quot;Accuracy with data injection: {accuracy:.2f}&quot;) #&gt;&gt; Accuracy with data injection: 0.95 So what is wrong with the previous code? In line \\(4\\) the entire dataset is normalized using MinMaxScaler(). In order to normalize the data, some parameters need to be learned. In this case, the minimum and maximum values of each feature. However, these parameters are being learned from the entire dataset. When the data is split into train and test sets (line \\(5\\)) some information from the train set may have leaked into the test set. This is because the minimum (or maximum) value of one of the features, could belong to an instance from the train set. But these values were used to normalize data that also belong to the test set. Typically, this leads to overestimation of performance metrics. In this case the accuracy was \\(0.95\\). The correct way to normalize the data is to first, learn the parameters from the train set and then, use those parameters to apply the transformation to the test set. In the following code, the data is first split into train and test sets. Then, the normalization parameters are learned (and applied) only from the train set (line \\(5\\)). The test set is then normalized with the previously learned parameters (line \\(6\\)). X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 123) scaler = MinMaxScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) knn = KNeighborsClassifier(n_neighbors = 3) knn.fit(X_train_scaled, y_train) y_pred = knn.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) print(f&quot;Accuracy without data injection: {accuracy:.2f}&quot;) #&gt;&gt; Accuracy without data injection: 0.93 In this case, the accuracy was lower (\\(0.93\\)) however, this performance estimate represents better the performance at test time. You can avoid data injection by making sure data from the training set is not leaking into the test set. For example, duplicated instances, normalization parameters, and so on. "],["notshuffling.html", "Mistake 11: Not shuffling the training data", " Mistake 11: Not shuffling the training data Not shuffling the data can degrade the performance of some models. In machine learning, some models are sensitive to data ordering, specially those based on gradient optimization and/or batch learning such as neural networks and many of their variants like Convolutional Neural Networks, Autoencoders, Generative Adversarial Networks, and so on. Shuffling the data (rows) helps to avoid bias from data ordering, improve generalization, and stabilize parameter learning. Shuffling helps having more representative batches of the overall data distribution. Not all machine learning models are sensitive to data ordering. For example, KNN or tree based models like decision trees, random forest, and XGBoost are not affected by data ordering. Bear in mind that shuffling is not recommended in some cases and should be avoided when fitting time-dependent models such as ARIMA and Recurrent Neural Networks. In sequence-based data, temporal order is crucial and shuffling alters the time-dependencies. The following code loads the WISDM dataset. Figure 11.1 shows the first ten rows. import pandas as pd df = pd.read_csv(&quot;data/wisdm/WISDM_ar_v1.1_transformed.csv&quot;) df.head(10) Figure 11.1: First rows of WISDM dataset. Here, you can see that the rows are sorted by user. You can easily shuffle the rows with the sample() function as follows. shuffled_df = df.sample(frac = 1, random_state = 1234).reset_index(drop=True) shuffled_df.head(10) Figure 11.2: First rows of WISDM dataset after shuffling. Figure 11.2 shows the result of shuffling the rows. The frac = 1 parameter indicates the fraction of data to be sampled without replacement. In this case, we want all data back so it is set to \\(1\\). When splitting the data using train_test_split(), the data will also be returned in a random order. As a good practice, the train set should be shuffled before fitting models that assume independence between instances. "],["savingresults.html", "Mistake 12: Not saving the results", " Mistake 12: Not saving the results Not saving the results after running an experiment. Sometimes running an experiment can take considerable time and the next step is to analyze its results. Often, I find myself adding new analyses and generating new plots from my previous results. However, from time to time I realize that I am not saving my results in an experiment that has already been running for several hours. Thus, I need to stop it, write code to save the results and start again. It is a good idea to write code to save your results from the beginning. By results, I mean any information that you may need to analyze in the future including: The models’ predictions and ground truth. Experiment configuration: hyper-parameters, iterations, description, etc. The trained models. Date, start, and end time of the experiment. Any other important information. The following code shows how to save a trained model with the WINE dataset so you can reuse it later on. tree = DecisionTreeClassifier(random_state = 123) tree.fit(X_train, y_train) import pickle # Save the model to hard drive. pickle.dump(tree, open(&#39;tree.pickle&#39;, &#39;wb&#39;)) # Load the model and make predictions. loaded_tree = pickle.load(open(&#39;tree.pickle&#39;, &#39;rb&#39;)) predictions = loaded_tree.predict(X_test) print(f&quot;Accuracy: {accuracy_score(y_test, predictions):.3f}&quot;) #&gt;&gt; Accuracy: 0.865 In this case I used the ‘pickle’ library to persist the trained model. However, there are other alternatives depending on your use case like ‘ONNX’, ‘joblib’ and so on (scikit-learn-model-persistence 2025). It is recommendeded to save your experiments’ results for further analysis. References scikit-learn-model-persistence. 2025. Model Persistence. https://scikit-learn.org/stable/model_persistence.html. "],["notparallelizing.html", "Mistake 13: Not parallelizing", " Mistake 13: Not parallelizing Not parallelizing your models when having multiple CPU cores. Not parallelizing your code is not critical in the sense that it will not affect your results. However, I am including it here because it is a mistake not to save time (since time is valuable) when you have the resources (CPU cores) to run things faster. Having multiple CPUs/cores in your computer will not automatically make your code run faster. You need to explicitly parallelize it. Making parallel programs is not an easy task (and not every algorithm can be parallelized). Fortunately, many scikit-learn models have been parallelized and it is as easy as setting one parameter to take full advantage of this feature! This parameter is n_jobs. By default, n_jobs=None in which case it will only create one job when calling fit() and predict(). Setting it to \\(-1\\) will use all the available processors/cores in your computer. However, I do not recommend using all your cores but leaving some of them free to work on other processes (operating system, browser, background applications, etc.), otherwise your computer may slow down. For example, my CPU has \\(20\\) cores. So I set n_jobs=15 to leave the remaining \\(5\\) cores to execute other processes. The following example uses the CALIFORNIA-HOUSING dataset which has \\(20640\\) instances and \\(8\\) features. The target value is the median house value, thus, we will train a regression Random Forest. from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error import time df = fetch_california_housing() X_train, X_test, y_train, y_test = train_test_split(df.data, df.target, test_size = 0.2, random_state = 123) rf = RandomForestRegressor(random_state = 123) start = time.time() rf.fit(X_train, y_train) y_pred = rf.predict(X_test) end = time.time() print(f&quot;MAE={mean_absolute_error(y_test, y_pred):.3f}&quot; f&quot; time (seconds) {end-start:.2f}&quot;) #&gt;&gt; MAE=0.324 time (seconds) 26.49 After splitting the data, I am training a RandomForestRegressor() and making predictions on the test set. Then, the mean absolute error (MAE) is computed. Finally, the MAE and the time it took to train the model and make the predictions is printed. Since this dataset is fairly large, it took \\(26.49\\) seconds in my computer. By looking at the Windows task manager (Figure 13.1 you can see that the Python program was using around \\(5.8\\%\\) of the CPU and only \\(8\\%\\) was used overall. Now let’s see if we can do better than that. Figure 13.1: CPU usage with one job. In the following code snippet, I am setting the n_jobs parameter to \\(15\\) when creating the RandomForestRegressor() object. My computer has \\(20\\) cores, so I am leaving \\(5\\) free cores for other tasks. rf = RandomForestRegressor(random_state = 123, n_jobs = 15) start = time.time() rf.fit(X_train, y_train) y_pred = rf.predict(X_test) end = time.time() print(f&quot;MAE={mean_absolute_error(y_test, y_pred):.3f}&quot; f&quot; time (seconds) {end-start:.2f}&quot;) #&gt;&gt; MAE=0.324 time (seconds) 3.98 From the results, we can see that even though the performance was the same (\\(0.324\\)) it only took \\(3.98\\) seconds to complete! Figure 13.2 shows my task manager. Figure 13.2: CPU usage with \\(15\\) jobs. This time Python was using \\(41.3\\%\\) of the CPU by taking advantage of its cores. Keep in mind that the speed improvement as the number of jobs is increased is not linear. This is becasue there is an overhead when creating new processes and memory allocation tasks. Furthermore, it is not guaranteed that a cpu/core will be assigned to a single job. The operating system decides how to distribute the work load based on its scheduling algorithm. Many machine learning models in scikit-learn have the n_jobs parameter which referes to the number of jobs to run in parallel. "],["encodingintegers.html", "Mistake 14: Encoding categories as integers", " Mistake 14: Encoding categories as integers Chapter under construction… "],["datachanges.html", "Mistake 15: Forget data changes over time", " Mistake 15: Forget data changes over time Chapter under construction… "],["interuservariance.html", "Mistake 16: Ignoring inter-user variance", " Mistake 16: Ignoring inter-user variance Chapter under construction… "],["wastingunlabeled.html", "Mistake 17: Wasting unlabeled data", " Mistake 17: Wasting unlabeled data Chapter under construction… "],["appendixInstall.html", "A Setup Your Environment", " A Setup Your Environment The examples in this book were tested with Python 3.12.7 within an Anaconda (Jupyter Team 2025) environment. The source code is provided in the form of jupyter notebooks (Anaconda Team 2025). Each chapter (mistake) has its own related jupyter notebook file with .ipynb extension. The scikit-learn version used in this book was 1.6.1. The pandas version is 2.2.2. Be aware that the results and outputs may vary depending on the scikit-learn version even when setting the random seed. The following steps describe how to setup your environment. 1. Download and install Anaconda. Download the Anaconda distribution based on your operating system. You can get Anaconda here: https://www.anaconda.com/download 2. Create an environment. Open the Anaconda prompt and run the following commands. conda create -n mlmistakes python=3.12.7 scikit-learn=1.6.1 pandas=2.2.2 3. Activate the mlmistakes environment. To activate the environment, run the following code. activate mlmistakes 4. Install Jupyter notebooks. Run the following command to install Jupyter notebooks. conda install anaconda::jupyter 5. Run the notebooks. In the Anaconda prompt, change to the directory where the notebooks (.ipynb) are stored. In windows, you can use the cd command. To start Jupyter notebooks type jupyter notebook. References Anaconda Team. 2025. Anaconda. https://www.anaconda.com/. Jupyter Team. 2025. Jupyter. https://jupyter.org/. "],["appendixDatasets.html", "B Datasets B.1 CALIFORNIA-HOUSING B.2 DIAGNOSTIC B.3 DIGITS B.4 IRIS B.5 WINE B.6 WISDM", " B Datasets This section lists all the datasets used in this book. Each dataset specifies whether or not it is included in scikit-learn. Those that are not part of scikit-learn, are included along with the source code in the data\\ directory. B.1 CALIFORNIA-HOUSING Included in scikit-learn: Yes The CALIFORNIA-HOUSING dataset (Pace and Barry 1997) has \\(20640\\) instances and \\(8\\) features that describe different characteristics of houses such as average number of rooms, house age, average occupancy, etc. The response variable is the median house value. B.2 DIAGNOSTIC Included in scikit-learn: Yes The DIAGNOSTIC dataset (Wolberg et al. 1993) has \\(569\\) instances and \\(30\\) features extracted from images of a fine needle aspirate of a breast mass. The class indicates if the breast tumor is benign or malignant. B.3 DIGITS Included in scikit-learn: Yes The DIGITS dataset contains \\(1797\\) handwritten digits (\\(0\\dots9\\)) as \\(8\\times8\\) images. This is a subset of the original data (Alpaydin and Kaynak 1998). B.4 IRIS Included in scikit-learn: Yes The IRIS dataset (Fisher 1936) includes information about 150 plants divided into three categories (‘setosa’, ‘virginica’, and ‘versicolor’). B.5 WINE Included in scikit-learn: Yes The WINE dataset (Aeberhard and Forina 1992) has \\(178\\) instances of wine chemical analyses. Each one is categorized into one of three classes (three different cultivators in Italy). B.6 WISDM Included in scikit-learn: No This dataset is called WISDM and was made available by Kwapisz, Weiss, and Moore (2010). The dataset includes \\(6\\) different activities: ‘walking’, ‘jogging’, ‘walking upstairs’, ‘walking downstairs’, ‘sitting’, and ‘standing’. The data was collected by \\(36\\) volunteers with the accelerometer of an Android phone located in the users’ pants pocket and with a sampling rate of \\(20\\) Hz. References Aeberhard, Stefan, and M. Forina. 1992. “Wine.” UCI Machine Learning Repository. https://doi.org/10.24432/C5PC7J. Alpaydin, E., and C. Kaynak. 1998. “Optical Recognition of Handwritten Digits.” UCI Machine Learning Repository. https://doi.org/10.24432/C50P49. Fisher, Ronald A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7 (2): 179–88. Kwapisz, Jennifer R., Gary M. Weiss, and Samuel A. Moore. 2010. “Activity Recognition Using Cell Phone Accelerometers.” In Proceedings of the Fourth International Workshop on Knowledge Discovery from Sensor Data (at KDD-10), Washington DC. Pace, R Kelley, and Ronald Barry. 1997. “Sparse Spatial Autoregressions.” Statistics &amp; Probability Letters 33 (3): 291–97. Wolberg, William., Olvi. Mangasarian, Nick. Street, and W. Street. 1993. “Breast Cancer Wisconsin (Diagnostic).” UCI Machine Learning Repository. https://doi.org/10.24432/C5DW2B. "],["citing-this-book.html", "Citing this Book", " Citing this Book If you found this book useful, you can consider citing it like this: Garcia-Ceja, Enrique. &quot;MOST COMMON MISTAKES IN MACHINE LEARNING AND HOW TO AVOID THEM: With Examples in Python&quot;, 2025. https://enriquegit.github.io/most-common-ml-mistakes BibTeX: @book{GarciaCejaBook, title = {MOST COMMON MISTAKES IN MACHINE LEARNING AND HOW TO AVOID THEM: With Examples in Python}, author = {Enrique Garcia-Ceja}, year = {2025}, note = {\\url{https://enriquegit.github.io/most-common-ml-mistakes}} } "],["references.html", "References", " References Aeberhard, Stefan, and M. Forina. 1992. “Wine.” UCI Machine Learning Repository. https://doi.org/10.24432/C5PC7J. Alpaydin, E., and C. Kaynak. 1998. “Optical Recognition of Handwritten Digits.” UCI Machine Learning Repository. https://doi.org/10.24432/C50P49. Anaconda Team. 2025. Anaconda. https://www.anaconda.com/. Fisher, Ronald A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7 (2): 179–88. Guigui14460. 2021. Differences in Scores Between Two Different Versions. https://github.com/scikit-learn/scikit-learn/issues/20042. Jupyter Team. 2025. Jupyter. https://jupyter.org/. Kwapisz, Jennifer R., Gary M. Weiss, and Samuel A. Moore. 2010. “Activity Recognition Using Cell Phone Accelerometers.” In Proceedings of the Fourth International Workshop on Knowledge Discovery from Sensor Data (at KDD-10), Washington DC. Pace, R Kelley, and Ronald Barry. 1997. “Sparse Spatial Autoregressions.” Statistics &amp; Probability Letters 33 (3): 291–97. scikit-learn-model-persistence. 2025. Model Persistence. https://scikit-learn.org/stable/model_persistence.html. Wolberg, William., Olvi. Mangasarian, Nick. Street, and W. Street. 1993. “Breast Cancer Wisconsin (Diagnostic).” UCI Machine Learning Repository. https://doi.org/10.24432/C5DW2B. ydata-profiling team. 2025. Ydata-Profiling Python Package. https://github.com/ydataai/ydata-profiling. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
