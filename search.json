[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"UPDATE 2020-12-27: Chapter 10 anomaly detection added!Thanks support!! book now list Shared Last Week KDnuggets.\nfree html version book. pdf e-book version can purchased book aims provide introduction machine learning concepts algorithms applied diverse set behavior analysis problems. focuses practical aspects solving problems based data collected sensors stored electronic records. included examples demonstrate perform several tasks involved data analysis pipeline : data exploration, visualization, preprocessing, representation, model training/validation, . , using R programming language real-life datasets.content find includes, :Build supervised machine learning models predict indoor locations based WiFi signals, recognize physical activities smartphone sensors 3D skeleton data, detect hand gestures accelerometer signals, .Use unsupervised learning algorithms discover criminal behavioral patterns.Program ensemble learning methods use multi-view stacking fuse signals heterogeneous data sources.Train deep learning models neural networks classify muscle activity electromyography signals CNNs detect smiles images.Evaluate performance models traditional multi-user settings.Train anomaly detection models Isolation Forests autoencoders detect abnormal fish trajectories.much !accompanying source code examples available https://github.com/enriquegit/behavior-code. book written R bookdown package1 developed Yihui Xie. front cover summary-comics illustrated Vance Capley.front cover depicts two brothers (Biås Biranz) seems typical weekend. exploring enjoying nature usual. don’t know lives change turning back. Suddenly, Biranz spots strange object approaching . makes way rocks, entire figure revealed. brothers never seen anything like . closest similar image brains hand-sized carnivorous plant saw botanical garden school visit several years ago. Without warning, creature releases load spores air. Today, breeze brothers’ side spores quickly enclose . seconds exposed, bodies start paralyze. Moments later, can barely move pupils. creature’s multiple stems begin inflate sudden, multiple thorns shot. Horrified incapable, brothers can witness thorns approach bodies can even hear air cut sharp thorns. point, waiting worst. …haven’t felt impact. time just stopped? —thorns repelled appears bionic dolphin emitting type ultrasonic waves. One projectiles managed dodge sound defense heading directly Biås. flying almost one mile sea level, eagle aims elusive thorn destroys surgical precision. However, creature persistent attacks. brothers escape crossfire battle?. name Enrique researcher SINTEF. Please feel free e-mail questions/comments/feedback, etc.e-mail:\ne_g_mxwebsite: http://www.enriquegc.com\nlive document keep updating adding new content.html version book licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Automatic behavior monitoring technologies becoming part everyday lives thanks advances sensors machine learning. automatic analysis understanding behavior applied solve problems several fields, including health care, sports, marketing, ecology, security, psychology, name . book provides practical introduction machine learning methods applied behavior analysis R programming language. book assume previous knowledge machine learning. familiar basics R knowledge basic statistics high school-level mathematics beneficial.","code":""},{"path":"preface.html","id":"supplemental-material","chapter":"Preface","heading":"Supplemental Material","text":"Supplemental material consists examples’ code datasets. source code examples can downloaded https://github.com/enriquegit/behavior-code. Instructions set code get datasets Appendix . reference utilized datasets Appendix ??.","code":""},{"path":"preface.html","id":"conventions","chapter":"Preface","heading":"Conventions","text":"DATASET names written uppercase italics. Functions referred name followed parenthesis omitting arguments, example: myFunction(). Class labels written italics single quotes: ‘label1’. following icons used provide additional contextual information:folder icon appear beginning section (applicable) indicate scripts used corresponding examples.","code":""},{"path":"preface.html","id":"acknowledgments","chapter":"Preface","heading":"Acknowledgments","text":"want thank Michael Riegler, Jaime Mondragon y Ariana, Viviana M., Linda Sicilia, Anton Aguilar, Aleksander Karlsen, former master’s PhD. advisor Ramon F. Brena, colleagues SINTEF.examples book rely heavily datasets. want thank people made datasets used publicly available. want thank Vance Capley brought life front cover comic illustrations.","code":""},{"path":"intro.html","id":"intro","chapter":"Mistake 1: Introduction to Machine Learning","heading":"Mistake 1: Introduction to Machine Learning","text":"Living organisms constantly sensing analyzing surrounding environment. includes inanimate objects also living entities. objective making decisions taking actions, either consciously unconsciously. see someone running, react differently depending whether stadium bank. time, may also analyze cues runner’s facial expressions, clothes, items, reactions people around us. Based aggregated information, can decide react behave. supported organisms’ sensing capabilities decision-making processes (brain /chemical reactions). Understanding environment others behave crucial conducting everyday life activities provides support tasks. , behavior? Cambridge dictionary defines behavior :“way person, animal, substance, etc. behaves particular situation particular conditions”.Another definition dictionary.com :“observable activity human animal.”definitions similar include humans animals. Following definitions, book focus automatic analysis human animal behaviors. three main reasons one may want analyze behaviors automatic manner:React. biological artificial agent (combination ) can take actions based happening surrounding environment. example, suspicious behavior detected airport, preventive actions can triggered security systems corresponding authorities. Without possibility automate detection system, infeasible implement practice. Just imagine trying analyze airport traffic hand.React. biological artificial agent (combination ) can take actions based happening surrounding environment. example, suspicious behavior detected airport, preventive actions can triggered security systems corresponding authorities. Without possibility automate detection system, infeasible implement practice. Just imagine trying analyze airport traffic hand.Understand. Analyzing behavior organism can help us understand associated behaviors processes answer research questions. example, Williams et al. (2020) found Andean condors (heaviest soaring bird) flap wings \\(1\\%\\) total flight time. one cases, condor flew \\(\\approx 172\\) km without flapping. findings result analyzing birds’ behavior data recorded bio-logging devices. book, several examples make use inertial devices studied.Understand. Analyzing behavior organism can help us understand associated behaviors processes answer research questions. example, Williams et al. (2020) found Andean condors (heaviest soaring bird) flap wings \\(1\\%\\) total flight time. one cases, condor flew \\(\\approx 172\\) km without flapping. findings result analyzing birds’ behavior data recorded bio-logging devices. book, several examples make use inertial devices studied.\nFigure 1.1: Andean condor. source: wikipedia.\nDocument/Archive. Finally, may want document certain behaviors future use. evidence purposes maybe clear information can used now may come handy later. Based archived information, one gain new knowledge future use react (take decisions/actions). example, document nutritional habits (eat, often, etc.). health issue, specialist use historical information make precise diagnosis propose actions.behaviors can used proxy understand behaviors, states, /processes. example, detecting body movement behaviors job interview serve basis understand stress levels. Behaviors can also modeled composition lower-level behaviors. chapter ??, method called Bag Words can used decompose complex behaviors set simpler ones presented.order analyze monitor behaviors, need way observe . Living organisms use available senses eyesight, hearing, smell, echolocation (bats, dolphins), thermal senses (snakes, mosquitoes), etc. case machines, need sensors accomplish approximate tasks. example, RGB thermal cameras, microphones, temperature sensors, .reduction size sensors allowed development powerful wearable devices. Wearable devices electronic devices worn user, usually accessories embedded clothes. Examples wearable devices smartphones, smartwatches, fitness bracelets, actigraphy watches, etc. devices embedded sensors allow monitor different aspects user activity levels, blood pressure, temperature, location, name . Examples sensors can found devices accelerometers, magnetometers, gyroscopes, heart rate, microphones, Wi-Fi, Bluetooth, Galvanic Skin Response (GSR), etc.Several sensors initially used specific purposes. example, accelerometers smartphones intended used gaming detecting device’s orientation. Later, people started propose implement new use cases activity recognition (Shoaib et al. 2015) fall detection. magnetometer, measures earth’s magnetic field, mainly used map applications determine orientation device, later, found can also used indoor location purposes (Brena et al. 2017).general, wearable devices proven success tracking different types behaviors physical activity, sports activities, location, even mental health states (Garcia-Ceja et al. 2018). sensors generate lot raw data, task process analyze . hand becomes impossible given large amounts available formats data available. Thus, book, several machine learning methods introduced extract analyze different types behaviors data.\nnext section begins introduction machine learning. rest chapter introduce required machine learning concepts start analyzing behaviors chapter ??.","code":""},{"path":"intro.html","id":"what-is-machine-learning","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.1 What is Machine Learning?","text":"can think Machine Learning set computational algorithms automatically find useful patterns relationships data. , keyword automatic. trying solve problem, one can hard-code predefined set rules, example, chained -else conditions. instance, want detect object picture orange pear, can something like:simple rule work well job. Imagine now boss tells system needs recognize green apples well. previous rule longer work, need include additional rules thresholds. hand, machine learning algorithm automatically learn rules based updated data. , need update data examples green apples “click” re-train button!result learning knowledge system can use solve new instances problem. case, show new image system able recognize type fruit. Figure 1.2 shows general idea.\nFigure 1.2: Overall Machine Learning phases.\nMachine learning methods rely three main building blocks:DataAlgorithmsModelsEvery machine learning method needs data learn . example fruits, need examples images type fruit want recognize. Additionally, need corresponding output types (labels) algorithm can learn associate image corresponding label.Typically, algorithm use data learn model. called learning training phase. learned model can used generate predictions presented new data. data used train models called train set. Since need way test model perform deployed real setting (production), also need known test set. test set used estimate model’s performance data never seen (presented section 1.5).","code":"if(number_green_pixels > 90%)\n  return \"pear\"\nelse\n  return \"orange\""},{"path":"intro.html","id":"taxonomy","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.2 Types of Machine Learning","text":"Machine learning methods can grouped different types. Figure 1.3 depicts categorization machine learning ‘types’. figure based (Biecek et al. 2012). \\(x\\) axis represents certainty labels \\(y\\) axis percent training data labeled. previous example, labels fruits’ names associated image.\nFigure 1.3: Machine learning taxonomy.\nfigure, four main types machine learning methods can observed:Supervised learning. case, \\(100\\%\\) training data labeled certainty labels \\(100\\%\\). like fruits example. every image used train system, respective label also known uncertainty label. expected output category (type fruit), called classification. Examples classification models (.k.classifiers) decision trees, k-nearest neighbors, random forest, neural networks, etc. output real number (e.g., predict temperature) called regression. Examples regression models linear regression, regression trees, neural networks, random forest, k-nearest neighbors, etc. Note models can used classification regression. supervised learning problem can formalized follows:\\[\\begin{equation}\n  f\\left(x\\right) = y\n  \\tag{1.1}\n\\end{equation}\\]\\(f\\) function maps input data \\(x\\) (example images) output \\(y\\) (types fruits). Usually, algorithm try learn best model \\(f\\) given data consisting \\(n\\) pairs \\((x,y)\\) examples. learning, algorithm access expected output/label \\(y\\) input \\(x\\). inference time, , want make predictions new examples, can use learned model \\(f\\) feed new input \\(x\\) obtain corresponding predicted value \\(y\\).Semi-supervised learning. case certainty labels \\(100\\%\\) training data labeled. Usually, scenario considers case small proportion data labeled. , dataset contains pairs examples form \\((x,y)\\) also examples \\(y\\) missing \\((x,?)\\). supervised learning, \\(x\\) \\(y\\) must present. hand, semi-supervised algorithms can learn even examples missing expected output \\(y\\). common situation real life since labeling data can expensive time-consuming. fruits example, someone needs tag every training image manually training model. Semi-supervised learning methods try extract information also unlabeled data improve models. Examples semi-supervised learning methods self-learning, co-training, tri-training, etc. (Triguero, García, Herrera 2013).Semi-supervised learning. case certainty labels \\(100\\%\\) training data labeled. Usually, scenario considers case small proportion data labeled. , dataset contains pairs examples form \\((x,y)\\) also examples \\(y\\) missing \\((x,?)\\). supervised learning, \\(x\\) \\(y\\) must present. hand, semi-supervised algorithms can learn even examples missing expected output \\(y\\). common situation real life since labeling data can expensive time-consuming. fruits example, someone needs tag every training image manually training model. Semi-supervised learning methods try extract information also unlabeled data improve models. Examples semi-supervised learning methods self-learning, co-training, tri-training, etc. (Triguero, García, Herrera 2013).Partially-supervised learning. generalization encompasses supervised semi-supervised learning. , examples uncertain (soft) labels. example, label fruit image instead “orange” “pear” vector \\([0.7, 0.3]\\) first element probability image corresponds orange second element probability ’s pear. Maybe image clear, beliefs person tagging images encoded probabilities. Examples models can used partially-supervised learning mixture models belief functions (Côme et al. 2009) neural networks.Partially-supervised learning. generalization encompasses supervised semi-supervised learning. , examples uncertain (soft) labels. example, label fruit image instead “orange” “pear” vector \\([0.7, 0.3]\\) first element probability image corresponds orange second element probability ’s pear. Maybe image clear, beliefs person tagging images encoded probabilities. Examples models can used partially-supervised learning mixture models belief functions (Côme et al. 2009) neural networks.Unsupervised learning. extreme case none training examples label. , dataset pairs \\((x,?)\\). Now, may wondering: labels, possible extract information data? answer yes. Imagine fruit images labels. try automatically group meaningful categories/groups. categories types fruits , .e., trying form groups images within category belong type. fruits example, infer true types visually inspecting images, many cases, visual inspection difficult formed groups may easy interpretation, still, can useful can used preprocessing step (like vector quantization). types algorithms find groups (hierarchical groups cases) called clustering methods. Examples clustering methods k-means, k-medoids, hierarchical clustering. Clustering algorithms unsupervised learning methods. Association rules, word embeddings, autoencoders examples unsupervised learning methods. people may disagree word embeddings autoencoders fully unsupervised methods practical purposes, categorization relevant.Unsupervised learning. extreme case none training examples label. , dataset pairs \\((x,?)\\). Now, may wondering: labels, possible extract information data? answer yes. Imagine fruit images labels. try automatically group meaningful categories/groups. categories types fruits , .e., trying form groups images within category belong type. fruits example, infer true types visually inspecting images, many cases, visual inspection difficult formed groups may easy interpretation, still, can useful can used preprocessing step (like vector quantization). types algorithms find groups (hierarchical groups cases) called clustering methods. Examples clustering methods k-means, k-medoids, hierarchical clustering. Clustering algorithms unsupervised learning methods. Association rules, word embeddings, autoencoders examples unsupervised learning methods. people may disagree word embeddings autoencoders fully unsupervised methods practical purposes, categorization relevant.Additionally, another type machine learning called Reinforcement Learning (RL) substantial differences previous ones. type learning rely example data previous ones stimuli agent’s environment. given point time, agent can perform action lead new state reward collected. aim learn sequence actions maximize reward. type learning covered book. good introduction topic can consulted here2.book mainly cover supervised learning problems specifically, classification problems. example, given set wearable sensor readings, want predict contextual information given user location, current activity, mood, . Unsupervised learning methods (clustering association rules) covered well chapter ??.","code":""},{"path":"intro.html","id":"terminology","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.3 Terminology","text":"section introduces basic terminology helpful rest book.","code":""},{"path":"intro.html","id":"tables","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.3.1 Tables","text":"Since data important ingredient machine learning, let’s start related terms. First, data needs stored/structured can easily manipulated processed. time, datasets stored tables R terminology, data frames. Figure 1.4 shows mtcars dataset stored data frame.\nFigure 1.4: Table/Data frame components.\nColumns represent variables rows represent examples also known instances data points. table, \\(5\\) variables mpg, cyl, disp, hp model (first column). example, first column name, still variable. row represents specific car model values per variable. machine learning terminology, rows commonly called instances whereas statistics often called data points observations. , terms used interchangeably.Figure 1.5 shows data frame iris dataset consists different kinds plants. Suppose interested predicting Species based variables. machine learning terminology, variable interest (one depends others) called class label classification problems. regression, often referred y. statistics, commonly known response, dependent, y variable, classification regression.machine learning terminology, rest variables called features attributes. statistics, called predictors, independent variables, just X. context, time easy identify dependent independent variables regardless used terminology.\nFigure 1.5: Table/Data frame components (cont.).\n","code":""},{"path":"intro.html","id":"variable-types","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.3.2 Variable Types","text":"working machine learning algorithms, following commonly used variable types. , talk variable types, refer programming-language-specific data types (int, boolean, string, etc.) general types regardless underlying implementation specific programming language.Categorical/Nominal: variables take values discrete set possible values (categories). example, categorical variable color can take values “red”, “green”, “black”, . Categorical variables ordering.Categorical/Nominal: variables take values discrete set possible values (categories). example, categorical variable color can take values “red”, “green”, “black”, . Categorical variables ordering.Numeric: Real values height, weight, price, etc.Numeric: Real values height, weight, price, etc.Integer: Integer values number rooms, age, number floors, etc.Integer: Integer values number rooms, age, number floors, etc.Ordinal: Similar categorical variables, take values set discrete values, ordering. example, low < medium < high.Ordinal: Similar categorical variables, take values set discrete values, ordering. example, low < medium < high.","code":""},{"path":"intro.html","id":"predictive-models","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.3.3 Predictive Models","text":"machine learning terminology, predictive model model takes input produces output. Classifiers Regressors predictive models. use terms classifier/model regressor/model interchangeably.","code":""},{"path":"intro.html","id":"pipeline","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.4 Data Analysis Pipeline","text":"Usually, data analysis pipeline consists several steps depicted Figure 1.6. complete list includes common steps. starts data collection. data exploration , results presented. steps can followed sequence, can always jump one step another one. fact, time end using iterative approach going one step (forward backward) needed.\nFigure 1.6: Data analysis pipeline.\nbig gray box bottom means machine learning methods can used steps just training evaluation. example, one may use dimensionality reduction methods data exploration phase plot data classification regression methods cleaning phase impute missing values. Now, let’s give brief description phases:Data exploration. step aims familiarize understand data can make informed decisions following steps. tasks involved phase include summarizing data, generating plots, validating assumptions, . phase can, example, identify outliers, missing values, noisy data points can cleaned next phase. Chapter ?? introduce data exploration techniques. Throughout book, also use data exploratory methods interested diving deeper topic, recommend check “Exploratory Data Analysis R” book Peng (2016).Data exploration. step aims familiarize understand data can make informed decisions following steps. tasks involved phase include summarizing data, generating plots, validating assumptions, . phase can, example, identify outliers, missing values, noisy data points can cleaned next phase. Chapter ?? introduce data exploration techniques. Throughout book, also use data exploratory methods interested diving deeper topic, recommend check “Exploratory Data Analysis R” book Peng (2016).Data cleaning. data exploration phase , can remove identified outliers, remove noisy data points, remove variables needed computation, .Data cleaning. data exploration phase , can remove identified outliers, remove noisy data points, remove variables needed computation, .Preprocessing. Predictive models expect data structured format satisfying constraints. example, several models sensitive class imbalances, .e., presence many instances given class small number instances classes. fraud detection scenarios, instances belong normal class just small proportion type ‘illegal transaction’. case, may want preprocessing try balance dataset. models also sensitive feature scale differences. example, variable weight kilograms another variable height centimeters. training predictive model, data needs prepared way models can get . Chapter ?? present common preprocessing steps.Preprocessing. Predictive models expect data structured format satisfying constraints. example, several models sensitive class imbalances, .e., presence many instances given class small number instances classes. fraud detection scenarios, instances belong normal class just small proportion type ‘illegal transaction’. case, may want preprocessing try balance dataset. models also sensitive feature scale differences. example, variable weight kilograms another variable height centimeters. training predictive model, data needs prepared way models can get . Chapter ?? present common preprocessing steps.Training evaluation. data preprocessed, can proceed train models. Furthermore, also need ways evaluate generalization performance new unseen instances. purpose phase try, fine-tune different models find one performs best. Later chapter, model evaluation techniques introduced.Training evaluation. data preprocessed, can proceed train models. Furthermore, also need ways evaluate generalization performance new unseen instances. purpose phase try, fine-tune different models find one performs best. Later chapter, model evaluation techniques introduced.Interpretation presentation results. purpose phase analyze interpret models’ results. can use performance metrics derived evaluation phase make informed decisions. may also want understand models work internally predictions derived.Interpretation presentation results. purpose phase analyze interpret models’ results. can use performance metrics derived evaluation phase make informed decisions. may also want understand models work internally predictions derived.","code":""},{"path":"intro.html","id":"trainingeval","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.5 Evaluating Predictive Models","text":"showing train machine learning model, section, like introduce process evaluating predictive model, part data analysis pipeline. applies classification regression problems. ’m starting topic recurring one every time work machine learning. also training lot models, need ways validate well.trained model (training set), , finding best function \\(f\\) maps inputs corresponding outputs, may want estimate good model solving particular problem presented examples never seen (part training set). estimate good model predicting output new examples called generalization performance.estimate generalization performance model, dataset usually divided train set test set. name implies, train set used train model (learn parameters) test set used evaluate/test generalization performance. need independent sets deploying models wild, presented new instances never seen . dividing dataset two subsets, simulating scenario test set instances never seen model training time performance estimate accurate rather used set train evaluate performance. two main validation methods differ way dataset divided: hold-validation k-fold cross validation.1) Hold-validation. method randomly splits dataset train test sets based predefined percentages. example, randomly select \\(70\\%\\) instances use train set use remaining \\(30\\%\\) examples test set. vary depending application amount data, typical splits \\(50/50\\) \\(70/30\\) percent train test sets, respectively. Figure 1.7 shows example dataset divided \\(70/30\\).\nFigure 1.7: Hold-validation.\n, train set used train (fit) model, test set evaluate well model performs new data. performance can measured using performance metrics accuracy classification problems. accuracy percent correctly classified instances.2) K-fold cross-validation. Hold-validation good way evaluate models lot data. However, many cases, data limited. cases, want make efficient use data. hold-validation, instance included either train test set. K-fold cross-validation provides way instances take part , test train set, thus making efficient use data.method consists randomly assigning instance one \\(k\\) folds (subsets) approximately size. , \\(k\\) iterations performed. iteration, one folds used test model remaining ones used train . fold used test set \\(k-1\\) times ’s used part train set. Typical values \\(k\\) \\(3\\), \\(5\\), \\(10\\). extreme case \\(k\\) equal total number instances dataset, called leave-one-cross-validation (LOOCV). Figure 1.7 shows example cross-validation \\(k=5\\).\nFigure 1.8: k-fold cross validation k=5 5 iterations.\ngeneralization performance computed taking average accuracy/error iteration.Hold-validation typically used lot available data models take significant time trained. hand, k-fold cross-validation used data limited. However, computational intensive since requires training \\(k\\) models.Validation set.predictive models require hyperparameter tuning. example, k-NN model requires set \\(k\\), number neighbors. decision trees, one can specify maximum allowed tree depth, among hyperparameters. Neural networks require even hyperparameter tuning work properly. Also, one may try different preprocessing techniques features. changes affect final performance. hyperparameter changes evaluated using test set, risk overfitting model. , making model specific particular data. Instead using test set fine-tune parameters, validation set needs used instead. Thus, dataset randomly partitioned three subsets: train/validation/test sets. train set used train model. validation set used estimate model’s performance trying different hyperparameters preprocessing methods. happy final model, use test set assess final generalization performance report. test set used . Remember want assess performance unseen instances. using k-fold cross validation, first, independent test set needs put aside. Hyperparameters tuned using cross-validation test set used end just estimate final performance.","code":""},{"path":"intro.html","id":"simple-classification-example","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.6 Simple Classification Example","text":"far, lot terminology concepts introduced. section, work practical example demonstrate concepts fit together. build (scratch) first classification regression models! Furthermore, learn evaluate generalization performance.Suppose dataset contains information felines including maximum speed km/hr specific type. sake example, suppose two variables ones can observe. types, consider two possibilities: ‘tiger’ ‘leopard’. Figure 1.9 shows first \\(10\\) instances (rows) dataset.\nFigure 1.9: First 10 instances felines dataset.\ntable \\(2\\) variables: speed class. first one numeric variable. second one categorical variable. case, can take two possible values: ‘tiger’ ‘leopard’.dataset synthetically created illustration purposes, promise , mostly use real datasets.code reproduce example contained Introduction folder script file simple_model.R. script contains code used generate dataset. dataset stored data frame named dataset. Let’s start simple exploratory analysis dataset. detailed exploratory analysis methods presented chapter ??. First, can print data frame dimensions dim() function.output tells us data frame \\(100\\) rows \\(2\\) columns. Now may interested know \\(100\\) instances, many correspond tigers. can use table() function get information.can see \\(50\\) instances type ‘leopard’ also \\(50\\) instances type ‘tiger’. fact, dataset intentionally generated. next thing can compute summary statistics column. R already provides convenient function purpose. Yes, summary() function.Since speed numeric variable, summary() computes statistics like mean, min, max, etc. class variable factor. Thus, returns row counts instead. R, categorical variables usually encoded factors. similar string, R treats factors special way. can already appreciate previous code snippet summary function returned class counts.many ways can explore dataset, now, let’s assume already feel comfortable good understanding data. Since dataset simple, won’t need data cleaning preprocessing.Now, imagine asked build model able predict type feline based observed attributes. case, thing can observe speed. task build function maps speed measurements classes. , want able predict type feline based fast runs. Based terminology presented section 1.3, speed feature variable class class variable.Based types machine learning presented section 1.2, one corresponds supervised learning problem , instance, respective label class can use train model. , specifically, since want predict category, classification problem.building classification model, worth plotting data. Let’s plot speeds tigers leopards.\nFigure 1.10: Feline speeds vertical dashed lines means.\n, omitted code building plot, included script. also added vertical dashed lines mean speeds two classes. plot, seems leopards faster tigers (exceptions). One thing can note data points grouped around mean values corresponding classes. , tiger data points closer mean speed tigers can observed leopards. course, exceptions instance closer mean opposite class. tigers may fast leopards. leopards may also slower average, maybe newborns old. Unfortunately, information best can use single feature speed. can use insights come simple model discriminates two classes based single feature variable.One thing can new instance want classify compute distance ‘center’ class predict class closest one. case, center mean value. can formally define model set \\(n\\) centrality measures \\(n\\) number classes (\\(2\\) example).\\[\\begin{equation}\n  M = \\{\\mu_1,\\dots ,\\mu_n\\}\n  \\tag{1.2}\n\\end{equation}\\]centrality measures (class means particular case) called parameters model. Training model consists finding optimal parameters allow us achieve best performance new instances part training data. cases, need algorithm find parameters. example, algorithm consists simply computing mean speed class. , class, sum speeds belonging class divide number data points class.parameters found, can start making predictions new data points. called inference prediction time. case, new data point arrives, can predict class computing distance \\(n\\) centrality measures \\(M\\) returning class closest one.following function implements training part model.first argument training data second argument centrality function want use (mean, default). function iterates class, computes centrality measure based speed, stores results named array called params returned end.time, training model involves passing training data additional hyperparameters specific model. case, centrality measure hyperparameter want use mean.Now function performs training, need another one performs actual inference prediction new data points. Let’s call one simple.classifier.predict(). first argument data frame instances want get predictions . second argument named vector parameters learned training. function return array predicted class instance newdata.function iterates row computes distance centrality measure returns name class closest one. distance computation done following line code:First, computes absolute difference speed centrality measure stored params , returns name one minimum. Now defined training prediction procedures, ready test classifier!section 1.5, two evaluation methods presented. Hold-k-fold cross-validation. methods allow estimate model perform new data. Let’s first start hold-validation.First, need split data two independent sets. use \\(70\\%\\) data train classifier remaining \\(30\\%\\) test . following code splits dataset trainset testset.sample() function used select integer numbers random \\(1\\) \\(n\\), \\(n\\) total number data points dataset. randomly selected data points ones go train set. Thus, size argument tell function return \\(70\\) numbers correspond \\(70\\%\\) total since dataset \\(100\\) instances.Now ’s time test functions. can train model using trainset calling previously defined function simple.model.train().training model, print learned parameters. case, mean tiger \\(48.88\\) leopard, \\(54.58\\). parameters, can start making predictions test set! pass test set newly-learned parameters function simple.classifier.predict().predict function returns predictions instance test set. can use head() function print first predictions. first two instances classified tigers, third one leopard, .good predictions? Since know true classes (also known ground truth) test set, can compute performance. case, compute accuracy, percentage correct classifications. Note use class information making predictions, used speed. pretended didn’t true class. use true class evaluate model’s performance.can compute accuracy counting many predictions equal true classes divide total number points test set. case, test accuracy \\(83.0\\%\\). Congratulations! trained evaluated first classifier.also good idea compute performance train set used train model.train accuracy \\(85.7\\%\\). expected, higher test accuracy. Typically, report performance test set, can use performance train set look signs /-fitting covered following sections.","code":"\n# Print number of rows and columns.\ndim(dataset)\n#> [1] 100   2\n# Count instances in each class.\ntable(dataset$class)\n#> leopard   tiger \n#>      50      50 \n# Compute some summary statistics.\nsummary(dataset)\n#>      speed           class   \n#>  Min.   :42.96   leopard:50  \n#>  1st Qu.:48.41   tiger  :50  \n#>  Median :51.12               \n#>  Mean   :51.53               \n#>  3rd Qu.:53.99               \n#>  Max.   :61.65               \n# Define a simple classifier that learns\n# a centrality measure for each class.\nsimple.model.train <- function(data, centrality=mean){\n  \n  # Store unique classes.\n  classes <- unique(data$class)\n  \n  # Define an array to store the learned parameters.\n  params <- numeric(length(classes))\n  \n  # Make this a named array.\n  names(params) <- classes\n  \n  # Iterate through each class and compute its centrality measure.\n  for(c in classes){\n    \n    # Filter instances by class.\n    tmp <- data[which(data$class == c),]\n    \n    # Compute the centrality measure.\n    centrality.measure <- centrality(tmp$speed)\n    \n    # Store the centrality measure for this class.\n    params[c] <- centrality.measure\n  }\n  \n  return(params)\n  \n}\n# Define a function that predicts a class\n# based on the learned parameters.\nsimple.classifier.predict <- function(newdata, params){\n  \n  # Variable to store the predictions of\n  # each instance in newdata.\n  predictions <- NULL\n  \n  # Iterate instances in newdata\n  for(i in 1:nrow(newdata)){\n    \n    instance <- newdata[i,]\n    \n    # Predict the name of the class which\n    # centrality measure is closest.\n    pred <- names(which.min(abs(instance$speed - params)))\n    \n    predictions <- c(predictions, pred)\n  }\n  \n  return(predictions)\n}\npred <- names(which.min(abs(instance$speed - params)))\n# Percent to be used as training data.\npctTrain <- 0.7\n\n# Set seed for reproducibility.\nset.seed(123)\n\nidxs <- sample(nrow(dataset),\n               size = nrow(dataset) * pctTrain,\n               replace = FALSE)\n\ntrainset <- dataset[idxs,]\n\ntestset <- dataset[-idxs,]\n# Train the model using the trainset.\nparams <- simple.model.train(trainset, mean)\n\n# Print the learned parameters.\nprint(params)\n#>    tiger  leopard \n#> 48.88246 54.58369\n# Predict classes on the test set.\ntest.predictions <- simple.classifier.predict(testset, params)\n\n# Display first predictions.\nhead(test.predictions)\n#> [1] \"tiger\"   \"tiger\"   \"leopard\" \"tiger\"   \"tiger\"   \"leopard\"\n# Compute test accuracy.\nsum(test.predictions == as.character(testset$class)) /\n  nrow(testset)\n#> [1] 0.8333333\n# Compute train accuracy.\ntrain.predictions <- simple.classifier.predict(trainset, params)\nsum(train.predictions == as.character(trainset$class)) /\n  nrow(trainset)\n#> [1] 0.8571429"},{"path":"intro.html","id":"k-fold-cross-validation-example","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.6.1 K-fold Cross-Validation Example","text":"Now, let’s see k-fold cross-validation can implemented test classifier. choose \\(k=5\\). means \\(5\\) independent sets going generated \\(5\\) iterations run., can use sample() function. time want select random integers \\(1\\) \\(k\\). total number integers equal total number instances \\(n\\) entire dataset. Note time set replace = TRUE since \\(k < n\\) need pick repeated numbers. number represent fold instance belongs . , need make sure instance belongs one sets. , guaranteeing assigning instance single fold number. can use table() function print many instances ended fold. , see folds contain \\(17\\) \\(23\\) instances.K-fold cross-validation consists iterating \\(k\\) times. iteration, select one folds function test set remaining folds used train set. can train model train set evaluate test set. end, report average accuracy across folds.test mean accuracy across \\(5\\) folds \\(\\approx 83\\%\\) similar accuracy estimated hold-validation.Note section 1.5 validation set also mentioned. one useful want fine-tune model /try different preprocessing methods data. case using hold-validation, may want split data three sets: train/validation/test sets. , train model using train set estimate performance using validation set. can fine-tune model. example, , instead mean centrality measure, can try use median measure performance validation set. pleased settings, estimate final performance model test set .One benefits machine learning allows us find patterns based data freeing us program hard-coded rules. means scalable flexible code. reason, now, instead \\(2\\) classes needed add another class, example, jaguar, thing need update database retrain model. don’t need modify internals algorithms. update based data.","code":"\n# Number of folds.\nk <- 5\n\nset.seed(123)\n\n# Generate random folds.\nfolds <- sample(k, size = nrow(dataset), replace = TRUE)\n\n# Print how many instances ended up in each fold.\ntable(folds)\n#> folds\n#>  1  2  3  4  5 \n#> 21 20 23 17 19 \n# Variable to store accuracies on each fold.\ntest.accuracies <- NULL\ntrain.accuracies <- NULL\n\nfor(i in 1:k){\n  testset <- dataset[which(folds == i),]\n  trainset <- dataset[which(folds != i),]\n  \n  params <- simple.model.train(trainset, mean)\n  test.predictions <- simple.classifier.predict(testset, params)\n  train.predictions <- simple.classifier.predict(trainset, params)\n  \n  # Accuracy on test set.\n  acc <- sum(test.predictions == \n               as.character(testset$class)) /\n    nrow(testset)\n  \n  test.accuracies <- c(test.accuracies, acc)\n  \n  # Accuracy on train set.\n  acc <- sum(train.predictions == \n               as.character(trainset$class)) /\n    nrow(trainset)\n  \n  train.accuracies <- c(train.accuracies, acc)\n}\n\n# Print mean accuracy across folds on the test set.\nmean(test.accuracies)\n#> [1] 0.829823\n\n# Print mean accuracy across folds on the train set.\nmean(train.accuracies)\n#> [1] 0.8422414"},{"path":"intro.html","id":"simple-regression-example","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.7 Simple Regression Example","text":"opposed classification models aim predict category, regression models predict numeric values. exemplify , can use felines dataset time can try predict speed based type feline. class column treated feature variable speed response variable. Since one predictor, categorical, best thing can implement regression model predict mean speed depending class.Recall classification, learned parameters consisted means class. Thus, can reuse training function simple.model.train(). need define new predict function returns speed based class. opposite classification (return class based speed).simple.regression.predict() function iterates instance newdata returns mean speed params corresponding class., can validate model using hold-validation. train set \\(70\\%\\) instances remaining used test set., reused previous function simple.model.train() learn parameters print . can use parameters infer speed. test instance belongs class ‘tiger’ return \\(48.88\\). class ‘leopard’ return \\(54.58\\).Since numeric predictions, use accuracy classification evaluate performance. One way evaluate well predictions computing mean absolute error (MAE). measure tells , average, much prediction deviates true value. computed subtracting prediction real value taking absolute value: \\(|predicted - realValue|\\). can visualized Figure 1.11. distances true predicted values errors MAE average errors.\nFigure 1.11: Prediction errors.\ncan use following code compute MAE:MAE test set \\(2.56\\). , average, simple model deviation \\(2.56\\) km/hr. respect true values, bad. can also compute MAE train set.MAE train set \\(2.16\\), better test set MAE (small MAE values preferred). Now, built, trained, evaluated regression model!simple example, illustrates basic idea regression differs classification. also shows performance regression models typically evaluated MAE opposed accuracy used classification. chapter ??, advanced methods neural networks introduced, can used solve regression problems.section, gone several data analysis pipeline phases. simple exploratory analysis data built, trained, validated models perform classification regression. Finally, estimated overall performance models presented results. , coded models scratch, practice, typically use models already implemented tested. , hope examples given feeling work machine learning.","code":"\n# Define a function that predicts speed\n# based on the type of feline.\nsimple.regression.predict <- function(newdata, params){\n  \n  # Variable to store the predictions of\n  # each instance in newdata.\n  predictions <- NULL\n  \n  # Iterate instances in newdata\n  for(i in 1:nrow(newdata)){\n    \n    instance <- newdata[i,]\n    \n    # Return the mean value of the corresponding class stored in params.\n    pred <- params[which(names(params) == instance$class)]\n    \n    predictions <- c(predictions, pred)\n  }\n  \n  return(predictions)\n}\npctTrain <- 0.7\nset.seed(123)\nidxs <- sample(nrow(dataset),\n               size = nrow(dataset) * pctTrain,\n               replace = FALSE)\n\ntrainset <- dataset[idxs,]\ntestset <- dataset[-idxs,]\n\n# Reuse our train function.\nparams <- simple.model.train(trainset, mean)\n\nprint(params)\n#>    tiger  leopard \n#> 48.88246 54.5836\n# Predict speeds on the test set.\ntest.predictions <- \n  simple.regression.predict(testset, params)\n\n# Print first predictions.\nhead(test.predictions)\n#> 48.88246 54.58369 54.58369 48.88246 48.88246 54.58369 \n# Compute mean absolute error (MAE) on the test set.\nmean(abs(test.predictions - testset$speed))\n#> [1] 2.562598\n# Predict speeds on the train set.\ntrain.predictions <- \n  simple.regression.predict(trainset, params)\n\n# Compute mean absolute error (MAE) on the train set.\nmean(abs(train.predictions - trainset$speed))\n#> [1] 2.16097"},{"path":"intro.html","id":"underfitting-and-overfitting","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.8 Underfitting and Overfitting","text":"felines classification example, saw can separate two classes computing mean class. two-class problem, equivalent decision line two means (Figure 1.12). Everything right decision line closer mean corresponds ‘leopard’ everything left ‘tiger’. case, classification function vertical line, learning position line reduces classification error searched . implicitly estimated position line finding mean values classes.\nFigure 1.12: Decision line two classes.\nNow, imagine access speed also felines’ age. extra information help us reduce prediction error since age plays important role fast feline . Figure 1.13 (left) shows look like plot age x-axis speed y-axis. , can see , tigers leopards, speed seems increase age increases. , point, age increases speed begins decrease.Constructing classifier single vertical line work \\(2\\)-dimensional case \\(2\\) predictors. need complex decision boundary (function) separate two classes. One approach use line time allow line angle. Everything line classified ‘tiger’ everything else ‘leopard’. Thus, learning phase involves finding line’s position angle achieves smallest error.Figure 1.13 (left) shows possible decision line. Even though function complex vertical line, still produce lot misclassifications (clearly separate classes). called underfitting, , model simple able capture underlying data patterns.\nFigure 1.13: Underfitting overfitting.\nLet’s try complex function, example, curve. Figure 1.13 (middle) shows curve better job separating two classes fewer misclassifications still, \\(3\\) leopards misclassified tigers \\(1\\) tiger misclassified leopard. Can better ? Yes, just keep increasing complexity decision function.Figure 1.13 (right) shows complex function able separate two classes \\(100\\%\\) accuracy equivalently, \\(0\\%\\) error. However, problem. function learned accurately separate training data, likely well new test set. function became specialized data failed capture overall pattern. called overfitting. case, model starts ‘memorize’ train set instead finding general patterns applicable new unseen instances. choose model, best one one middle. Even perfect train data, better models evaluating new test data.Overfitting common problem machine learning. One way know model overfitting error train set low, high new set (can test validation set). Figure 1.14 illustrates idea. -simple models high error , train validation sets. complexity model increases, error sets reduced. , point, complexity model high gets specific train set fails perform well new independent set.\nFigure 1.14: Model complexity v.s. train validation error.\nexample, saw underfitting overfitting can affect generalization performance model classification setting can occur regression problems.several methods aim reduce overfitting many specific type model. example, decision trees (covered chapter ??), one way reduce overfitting limit depth build ensembles trees (chapter ??). Neural networks also highly prone overfitting since can complex millions parameters also several techniques reduce effect overfitting (chapter ??).","code":""},{"path":"intro.html","id":"bias-and-variance","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.9 Bias and Variance","text":"far, seen can train predictive models evaluate well new data (test/validation sets). main goal predictive models low error rate new data. Understanding source error can help us make informed decisions building predictive models. test error, also known generalization error predictive model can decomposed three components: bias, variance, noise.Noise. component related data nothing can . example, two instances values features different label.Bias. bias much average prediction differs true value. Note average keyword. means make assumption infinite (large) number train sets can generated, predictive model trained. average predictions models see much average differs true value.Variance. variance relates much predictions change given data point training model using different train set time.following picture graphically illustrates different scenarios high/low bias variance. center target represents true value small red dots predictions hypothetical test set.\nFigure 1.15: Error due bias variance.\nBias variance closely related underfitting overfitting. High variance sign overfitting. , model complex fit particular train set well. Every time trained different train set, train error low, likely generate different predictions test points much higher test error. Figure 1.16 illustrates relation overfitting high variance regression problem. Given feature \\(x\\), two models trained predict \\(y\\): ) complex model (top row), ii) simpler model (bottom row). models fitted two training sets (\\(\\) \\(b\\)) sampled distribution. complex model fits train data perfectly makes different predictions (big \\(\\Delta\\)) test point using different train set. simpler model fit train data well smaller \\(\\Delta\\) lower error test point well. Visually, function (red curve) complex model also varies lot across train sets whereas shapes simpler model functions look similar.\nFigure 1.16: High variance overfitting.\nhand, model simple, underfit causing highly biased results able capture input-output relationships. results high train error consequence high test error well.","code":""},{"path":"intro.html","id":"SummaryIntro","chapter":"Mistake 1: Introduction to Machine Learning","heading":"1.10 Summary","text":"chapter, several introductory machine learning concepts terminology introduced. concepts basis methods covered following chapters.Behavior can defined “observable activity human animal”.Three main reasons may want analyze behavior automatically discussed: react, understand, document/archive.One way observe behavior automatically use sensors /data.Machine Learning consists set computational algorithms automatically find useful patterns relationships data.three main building blocks machine learning : data, algorithms, models.main types machine learning supervised learning, semi-supervised learning, partially-supervised learning, unsupervised learning.R, data usually stored data frames. Data frames variables (columns) instances (rows). Depending task, variables can independent dependent.predictive model model takes input produces output. Classifiers regressors predictive models.data analysis pipeline consists several tasks including data collection, cleaning, preprocessing, training/evaluation, presentation results.Model evaluation can performed hold-validation k-fold cross-validation.Overfitting occurs model ‘memorizes’ training data instead finding useful underlying patterns.test error can decomposed noise, bias, variance.","code":""},{"path":"settingseed.html","id":"settingseed","chapter":"Mistake 2: Not setting a seed value","heading":"Mistake 2: Not setting a seed value","text":"working machine learning, time dealing non-deterministic functions. Non-deterministic means randomness involved process, typically, happens making call (directly indirectly) random number generator. example, splitting data train test sets, train_test_split() function scikit-learn. following code snippet loads wine dataset splits train test set. , prints first 10 classes train set.run previous code multiple times, get different results every time. fact, behavior expecting since data split randomly. point may wondering: error? considered mistake? reason categorize mistake circumstances, including publish results share code someone else, difficult get result . , isn’t contradictory? one hand, want randomly split data, hand, want get results every time. answer since random number generators truly random pseudo-random, , can achieve objectives. can simulate random process time make reproducible. , can set fixed seed value random generator. Thus, every time call function based random number generator produce result respect initial seed value. scikit-learn, can set seed value various functions random_state parameter.following code adds random_state = 123 parameter train_test_split() function. parameter accepts integer number.Now every time run previous code generate result based value random_state parameter. change value, different train/test split generated guaranteed get result use integer value.training model relies random number generators, , set random_state parameter well. example, RandomForestClassifier picks random features fit trees. following code crates random forest model sets random state.","code":"from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_wine\n\n# Load the wine dataset.\ndata = load_wine()\n\n# Divide into train and test sets.\nX_train, X_test, y_train, y_test = train_test_split(data.data,\n                                                    data.target,\n                                                    test_size=0.5)\n\n# Print the first classes in the train set.\nprint(y_train[0:10])#>> [1 0 1 1 0 2 1 1 1 2]from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_wine\n\n# Load the wine dataset.\ndata = load_wine()\n\n# Divide into train and test sets.\nX_train, X_test, y_train, y_test = train_test_split(data.data,\n                                                    data.target,\n                                                    test_size=0.5,\n                                                    random_state = 123)\n\n# Print the first classes in the train set.\nprint(y_train[0:10])#>> #>> 2 2 2 1 0 0 1 0 1 1from sklearn.ensemble import RandomForestClassifier\n\n# Instantiate a RandomForestClassifier and set its random state.\nrf = RandomForestClassifier(n_estimators=50, random_state=123)"},{"path":"accountingvariance.html","id":"accountingvariance","chapter":"Mistake 3: Not accounting for variance","heading":"Mistake 3: Not accounting for variance","text":"deploying model production, want sure using best model given use case. order find best model, may want compare different alternatives. demonstrate idea, use digits dataset consists \\(8x8\\) images flattened feature vector length 64. feature consists integer 0-16 representing pixel intensity. Figure 3.1 shows first digit class ‘0’.\nFigure 3.1: Digit 0\nLet’s compare DecisionTreeClassifier versus GaussianNB decide one better digits classification. sake simplicity, use accuracy performance metric.According results may conclude DecisionTreeClassifier best choice. However, may case initial conditions change little bit (e.g., slightly different training set), get completely different results. fact, change random_state=1234 random_state=123 train_test_split() time GaussianNB performs better. called variance. , results vary run run. , can decide model better long run? One way repeat experiment several times select model performs better average. Every time run experiment, randomly partition train test sets. way, every experiment slightly different train test sets. procedure called Monte Carlo cross-validation.following code runs 500 iterations computes average accuracy., can see GaussianNB better average, contrary thought running experiment one iteration.Figure 3.2 shows histogram accuracies. , can see overlap two models average, GaussianNB better.\nFigure 3.2: Histogram accuracy.\ndifference models’ accuracies can validated paired t-test.","code":"import numpy as np\nfrom sklearn.datasets import load_digits\ndata = load_digits()\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data.data,\n                                                    data.target,\n                                                    test_size = 0.5,\n                                                    random_state = 123)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ntree = DecisionTreeClassifier(random_state=1234)\nbayes = GaussianNB()\n\ntree.fit(X_train, y_train)\nbayes.fit(X_train, y_train)\n\npredictions_tree = tree.predict(X_test)\npredictions_bayes = bayes.predict(X_test)\n\nprint(f\"Decision Tree accuracy: {accuracy_score(y_test, predictions_tree):.3f}\")\nprint(f\"GaussianNB accuracy: {accuracy_score(y_test, predictions_bayes):.3f}\")#>> Decision Tree accuracy: 0.844\n#>> GaussianNB accuracy: 0.778n = 500\naccuracy_tree = []\naccuracy_bayes = []\n\nfor i in range(n):\n    X_train, X_test, y_train, y_test = train_test_split(data.data,\n                                                    data.target,\n                                                    test_size = 0.5,\n                                                    random_state = 123 + i)\n    tree = DecisionTreeClassifier(random_state=123)\n    bayes = GaussianNB()\n    \n    tree.fit(X_train, y_train)\n    bayes.fit(X_train, y_train)\n    predictions_tree = tree.predict(X_test)\n    predictions_bayes = bayes.predict(X_test)\n    accuracy_tree.append(accuracy_score(y_test, predictions_tree))\n    accuracy_bayes.append(accuracy_score(y_test, predictions_bayes))\n    \nprint(f\"Decision Tree accuracy: {np.mean(accuracy_tree):.3f}\")\nprint(f\"GaussianNB accuracy: {np.mean(accuracy_bayes):.3f}\")#>> Decision Tree accuracy: 0.829\n#>> GaussianNB accuracy: 0.837from scipy.stats import ttest_rel\nt_stat, p_value = ttest_rel(accuracy_tree, accuracy_bayes)\nprint(f\"p-value: {p_value:.10f}\")#>> p-value: 0.0000000054"},{"path":"datainjection.html","id":"datainjection","chapter":"Mistake 4: Data injection","heading":"Mistake 4: Data injection","text":"Data injection one common mistakes machine learning materializes many different ways. assessing generalization performance model, independent test set used compute performance metrics like accuracy, precision, f1-score,etc. keyword : independent. means trained model information set. However, many ways can inadvertently violate assumption ‘injecting’ information model test time. typically happens applying normalization class balancing methods data.\nFigure 4.1: Andean condor. source: wikipedia.\n","code":"if(number_green_pixels > 90%)\n  return \"pear\"\nelse\n  return \"orange\""},{"path":"appendixInstall.html","id":"appendixInstall","chapter":"A Setup Your Environment","heading":"A Setup Your Environment","text":"examples book tested R 4.0.2. can get latest R version official website: www.r-project.org/IDE, use RStudio (https://rstudio.com/) can use favorite one. code examples book rely datasets. following two sections describe get install datasets source code. want try examples, recommend follow instructions following two sections.last section includes instructions install Keras TensorFlow required libraries build train deep learning models. Deep learning covered chapter ??. , don’t need libraries.","code":""},{"path":"appendixInstall.html","id":"installing-the-datasets","chapter":"A Setup Your Environment","heading":"A.1 Installing the Datasets","text":"compressed file collection datasets used book can downloaded : https://github.com/enriquegit/behavior-datasets.Download datasets collection file (behavior_book_datasets.zip) extract local directory, example, C:/datasets/. compilation includes datasets. Due datasets large file sizes license restrictions, included collection set. can download separately. Even though dataset may included compiled set, still corresponding directory README file instructions get . following picture shows directory structure looks like PC.","code":""},{"path":"appendixInstall.html","id":"installing-the-examples-source-code","chapter":"A Setup Your Environment","heading":"A.2 Installing the Examples Source Code","text":"examples source code can downloaded : https://github.com/enriquegit/behavior-codeYou can get code using git familiar , click ‘Code’ button ‘Download zip’. , extract file local directory choice.directory chapter two additional directories: auxiliary_functions/ install_functions/.auxiliary_functions/ folder generic functions imported R scripts. directory, file called globals.R. Open file set variable datasets_path local path downloaded datasets. example, set :install_functions/ directory single script: install_packages.R. script can used install packages used examples (except Keras TensorFlow covered next section). script reads packages listed listpackages.txt tries install present. just convenient way install everything can always install package individually usual install.packages() method.running examples, assumed working directory actual script. example, want try indoor_classification.R, script located C:/code/Predicting Behavior Classification Models/ , working directory C:/code/Predicting Behavior Classification Models/. Windows, RStudio already opened, double-click R script, RStudio launched (set default program) working directory set.can check current working directory typing getwd() can set working directory setwd(). Alternatively, RStudio, can set working directory menu bar ‘Session’ -> ‘Set Working Directory’ -> ‘Source File Location’.","code":"\ndatasets_path <- \"C:/datasets\""},{"path":"appendixInstall.html","id":"running-shiny-apps","chapter":"A Setup Your Environment","heading":"A.3 Running Shiny Apps","text":"Shiny apps3 interactive applications written R. book includes shiny apps demonstrate concepts. Shiny apps file names start prefix shiny_ followed specific file name. ‘.Rmd’ extension others ‘.R’ extension. Regardless extension, run way. running shiny apps, make sure installed packages shiny shinydashboard.run app, just open corresponding file RStudio. RStudio detect shiny app ‘Run Document’ ‘Run App’ button shown. Click button start app.","code":"\ninstall.packages(\"shiny\")\ninstall.packages(\"shinydashboard\")"},{"path":"appendixInstall.html","id":"installing-keras-and-tensorflow","chapter":"A Setup Your Environment","heading":"A.4 Installing Keras and TensorFlow","text":"TensorFlow two main versions. CPU GPU version. GPU version takes advantage capabilities video cards perform faster operations. examples book can run CPU version. following instructions apply CPU version. Installing GPU version requires platform-specific details. recommend first install CPU version want/need perform faster computations, , go GPU version.Installing Keras TensorFlow (CPU version) backend takes four simple steps:Windows, first install Anaconda4.Windows, first install Anaconda4.Install keras R package install.packages(\"keras\")Install keras R package install.packages(\"keras\")Load keras library(keras)Load keras library(keras)Run install_keras(). function install TensorFlow backend. don’t Anaconda installed, asked want install Miniconda.Run install_keras(). function install TensorFlow backend. don’t Anaconda installed, asked want install Miniconda.can test installation :first time session run TensorFlow related code CPU version, may get warning messages like following, can safely ignore.","code":"\nlibrary(tensorflow)\ntf$constant(\"Hello World\")\n#> tf.Tensor(b'Hello World', shape=(), dtype=string)\n#> tensorflow/stream_executor/platform/default/dso_loader.cc:55]\n#> Could not load dynamic library 'cudart64_101.dll';\n#> dlerror: cudart64_101.dll not found"},{"path":"citing-this-book.html","id":"citing-this-book","chapter":"Citing this Book","heading":"Citing this Book","text":"found book useful, can consider citing like :BibTeX:","code":"Garcia-Ceja, Enrique. \"Behavior Analysis with Machine Learning and R: A Sensors and Data Driven Approach\", 2020. http://behavior.enriquegc.com@book{GarciaCejaBook,\n  title = {Behavior Analysis with Machine Learning and {R}: A Sensors and Data Driven Approach},\n  author = {Enrique Garcia-Ceja},\n  year = {2020},\n  note = {\\url{http://behavior.enriquegc.com}}\n}"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
